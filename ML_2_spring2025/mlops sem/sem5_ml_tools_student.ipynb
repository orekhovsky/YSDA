{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **Seminar 5 - –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏**\n",
    "*Naumov Anton (Any0019)*\n",
    "\n",
    "*To contact me in telegram: @any0019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace ( https://huggingface.co ) - –æ–¥–∏–Ω –∏–∑ –≤–∞—à–∏—Ö –ª—É—á—à–∏—Ö –¥—Ä—É–∑–µ–π –∫–∞–∫ ML-—â–∏–∫–æ–≤\n",
    "\n",
    "–≠—Ç–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "–ù–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏, –∞ —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è—Ç—å –∏ —Ö–æ—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª–∏, –¥–∞—Ç–∞—Å–µ—Ç—ã, api-–∫–∏\n",
    "\n",
    "–¢–∞–∫–∂–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –∏–º–µ–µ—Ç —Å–µ—Ä—å—ë–∑–Ω—É—é –∏ –æ—á–µ–Ω—å —Å–∏–ª—å–Ω—É—é python-–±–∏–±–ª–∏–æ—Ç–µ–∫—É (–≤–µ—Ä–Ω–µ–µ —Ü–µ–ª–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –±–∏–±–ª–∏–æ—Ç–µ–∫) –¥–ª—è ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 python-–±–∏–±–ª–∏–æ—Ç–µ–∫–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£ HuggingFace –µ—Å—Ç—å —Ü–µ–ª—ã–π –Ω–∞–±–æ—Ä –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è ML\n",
    "\n",
    "–î–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏ ( https://huggingface.co/docs/hub/models-libraries ), –∏–∑ —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö:\n",
    "- transformers - –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å NLP\n",
    "- diffusers - –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–∫–∞–º–∏\n",
    "- PEFT - Parameter-Efficient Fine-Tuning (Lora)\n",
    "\n",
    "–î–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ ( https://huggingface.co/docs/hub/datasets-libraries ), –∏–∑ —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö:\n",
    "- datasets - –¥–∞—Ç–∞—Å–µ—Ç—ã :)\n",
    "\n",
    "–í —Ü–µ–ª–æ–º —ç—Ç–æ –¥–∞–∂–µ –±–ª–∏–∑–∫–æ –Ω–µ –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ ( https://github.com/huggingface ):\n",
    "- evaluate ( https://github.com/huggingface/evaluate ) - —Ä–∞–∑–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ / –±–µ–Ω—á–º–∞—Ä–∫–∏\n",
    "- accelerate ( https://github.com/huggingface/accelerate ) - multi-gpu –æ–±—É—á–µ–Ω–∏—è\n",
    "- optimum ( https://github.com/huggingface/optimum ) - –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
    "- ...\n",
    "\n",
    "sklearn –≤ –º–∏—Ä–µ DL :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers datasets evaluate scikit-learn accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Transformers - pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–æ–Ω—Ü–µ–ø—Ü–∏—è pipeline-–æ–≤ —Ç–∞–∫–æ–≤–∞, —á—Ç–æ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è 3 –≤–µ—â–∏ –≤ –æ–¥–Ω—É –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é:\n",
    "1. –ü—Ä–µ-–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, ...)\n",
    "2. –ú–æ–¥–µ–ª—å\n",
    "3. –ü–æ—Å—Ç-–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    task='sentiment-analysis',\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier(\"This model is nice!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier(\n",
    "    [\n",
    "        \"What an awful thing...\",\n",
    "        \"It's great in what it was designed for, but kinda awful that everything is done for me\",\n",
    "    ]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?classifier.postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlm_model = pipeline('fill-mask', model=\"bert-base-uncased\")\n",
    "mlm_model = pipeline(task='fill-mask', model=\"bert-base-cased\")\n",
    "MASK = mlm_model.tokenizer.mask_token\n",
    "\n",
    "for hypo in mlm_model(f\"Donald {MASK} is the president of the united states.\"):\n",
    "  print(f\"P={hypo['score']:.5f}\", hypo['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier, mlm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°—É—â–µ—Å—Ç–≤—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –ø–æ–¥ —Å–∞–º—ã–µ —Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ - –±—ã—Å—Ç—Ä–æ –Ω–∞–π—Ç–∏ –ª—é–±—ã–µ –º–æ–¥–µ–ª–∏: https://huggingface.co/models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "text = \"\"\"Almost two-thirds of the 1.5 million people who viewed this liveblog had Googled to discover\n",
    " the latest on the Rosetta mission. They were treated to this detailed account by the Guardian‚Äôs science editor,\n",
    " Ian Sample, and astronomy writer Stuart Clark of the moment scientists landed a robotic spacecraft on a comet \n",
    " for the first time in history, and the delirious reaction it provoked at their headquarters in Germany.\n",
    "  ‚ÄúWe are there. We are sitting on the surface. Philae is talking to us,‚Äù said one scientist.\n",
    "\"\"\"\n",
    "\n",
    "# –ó–∞–¥–∞—á–∞: –°–æ–∑–¥–∞–π—Ç–µ pipeline –¥–ª—è Named Entity Recognition (NER) –∑–∞–¥–∞—á–∏, –∏—â–∏—Ç–µ –º–æ–¥–µ–ª—å–∫–∏ –Ω–∞ —Ö–∞–±–µ\n",
    "#  - –ª–∏–±–æ –ø–æ —Ç–µ–∫—Å—Ç—É ner –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏\n",
    "#  - –ª–∏–±–æ –ø–æ –∑–∞–¥–∞—á–µ Token Classification\n",
    "ner_model = ...\n",
    "\n",
    "named_entities = ner_model(text)\n",
    "named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_entity = {item['word']: item['entity'] for item in named_entities}\n",
    "assert 'org' in word_to_entity.get('Guardian').lower() and 'per' in word_to_entity.get('Stuart').lower()\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Transformers - model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    "    \"Luke, I am your father.\",\n",
    "    \"Life is what happens when you're busy making other plans.\",\n",
    "]\n",
    "\n",
    "# —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤. \"pt\" - [p]y[t]orch tensors\n",
    "tokens_info = tokenizer(lines, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "for key in tokens_info:\n",
    "    print(key, tokens_info[key].shape, tokens_info[key], sep=\"\\n\", end=\"\\n\\n\")\n",
    "\n",
    "print(\"Detokenized:\")\n",
    "for i in range(2):\n",
    "    print(tokenizer.decode(tokens_info['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_for_analyse = \"some random text for deeper analysis + weird word Rutherfordium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in tokenizer(text_for_analyse).items():\n",
    "    print(key, value, sep=\"\\n\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(text_for_analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(text_for_analyse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(tokenizer.encode(text_for_analyse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(text_for_analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    tokenizer.all_special_ids,\n",
    "    tokenizer.all_special_tokens,\n",
    "    tokenizer.all_special_tokens_extended,\n",
    "    tokenizer.added_tokens_encoder,\n",
    "    tokenizer.added_tokens_decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    "    \"Luke, I am your father.\",\n",
    "    \"Life is what happens when you're busy making other plans.\",\n",
    "]\n",
    "\n",
    "tokens_info = tokenizer(lines, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens_info)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.layer[-1].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/datasets/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"fancyzhx/yelp_polarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"][0:5][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenize_function(ds[\"train\"][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = ds.map(tokenize_function, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1024))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "small_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"label\", \"attention_mask\"])\n",
    "dataloader = DataLoader(small_train_dataset, batch_size=4)\n",
    "res = next(iter(dataloader))\n",
    "\n",
    "for key, value in res.items():\n",
    "    print(key, value.shape, value, sep=\"\\n\", end=\"\\n-------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–º–µ–µ—Ç –º–Ω–æ–≥–æ —á–µ–≥–æ\n",
    "```python\n",
    "ds.rename_column(\"text\", \"unsplit_text\")  # –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏\n",
    "ds.cast_column(\"image\", Image(mode=\"RGB\"))  # –ø—Ä–∏–≤–æ–¥–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∫ –Ω—É–∂–Ω–æ–º—É –≤–∏–¥—É\n",
    "dataset.with_transform(transforms)  # –∞—É–≥—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ –±–µ–≥—É\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/evaluate/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute(predictions=[1, 2, 3, 4], references=[1, 1, 1, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute(predictions=[1, 2, 3, 4], references=[4, 3, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute(predictions=[1, 2, 3, 4], references=[1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Transformers - Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "?TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./my_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # lr_scheduler_kwargs={},\n",
    "    # warmup_ratio=0.03125,\n",
    "    # warmup_steps=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    log_level=\"error\",\n",
    "    # logging_dir=\"output_dir/runs/CURRENT_DATETIME_HOSTNAME\"  # –ª–æ–≥–∏ –¥–ª—è tensorboard (default)\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    # save_steps=1,\n",
    "    save_total_limit=2,\n",
    "    save_safetensors=True,  # safetensors –≤–º–µ—Å—Ç–æ torch.save / torch.load\n",
    "    save_only_model=False,  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º optimizer, shceduler, rng, ...\n",
    "    use_cpu=False,\n",
    "    seed=42,\n",
    "    # bf16=True,  # –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å bf16 –≤–º–µ—Å—Ç–æ fp32\n",
    "    eval_strategy=\"epoch\",\n",
    "    # eval_steps=32,\n",
    "    disable_tqdm=False,\n",
    "    load_best_model_at_end=False,\n",
    "    label_smoothing_factor=0.,\n",
    "    optim=\"adamw_torch\",\n",
    "    # optim_args=...,\n",
    "    # resume_from_checkpoint=...,\n",
    "    # auto_find_batch_size=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"This was not a good movie!\",\n",
    "    \"What an awesome place!\",\n",
    "    \"ewww\",\n",
    "]\n",
    "\n",
    "tokens_info = tokenizer(\n",
    "    texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model.cpu()\n",
    "with torch.no_grad():\n",
    "    out = model(**tokens_info)\n",
    "    probs = torch.nn.functional.softmax(out.logits, dim=-1)\n",
    "    for text, prob in zip(texts, probs.tolist()):\n",
    "        print(\n",
    "            f\"Text: `{text}`\\nPrediction (prob): \"\n",
    "            f\"positive={round(prob[0], 3)} ; \"\n",
    "            f\"negative={round(prob[1], 3)}\",\n",
    "            end=\"\\n\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. StreamLit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StreamLit - –ø—Ä–æ—Å—Ç–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: streamlit in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (1.44.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (24.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (2.2.1)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (10.3.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (17.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (6.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from altair<6,>=4.0->streamlit) (1.34.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->streamlit) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\1\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "streamlit hello  # –¥–µ–º–æ —Å –∫–æ–¥–æ–º –æ—Ç —Å–∞–º–æ–≥–æ streamlit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ streamlit —Å—Ç—Ä–æ—è—Ç—Å—è –ø–æ—Å—Ç—Ä–æ—á–Ω–æ, –∞ –Ω–µ –æ—Ç –º–∞–∫–µ—Ç–∞\n",
    "\n",
    "–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:\n",
    "1. –ò—Å–ø–æ–ª—å–∑—É–π —Å–∫—Ä–∏–ø—Ç—ã –Ω–∞ Python. –ü–æ—Å—Ç—Ä–æ—á–Ω–æ —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ –∏ —Ä–∞—Å—à–∏—Ä—è–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è Streamlit.\n",
    "2. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–π –≤–∏–¥–∂–µ—Ç—ã –∫–∞–∫ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ. –í–∏–¥–∂–µ—Ç—ã - —ç—Ç–æ —ç–ª–µ–º–µ–Ω—Ç—ã –≤–≤–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏ Streamlit. –û–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –≤–∏–¥–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π –≤–≤–æ–¥–∞, —Ñ–ª–∞–∂–∫–æ–≤, –ø–æ–ª–∑—É–Ω–∫–æ–≤ –∏ —Ç.–¥.\n",
    "3. –ü–æ–≤—Ç–æ—Ä–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–π –¥–∞–Ω–Ω—ã–µ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è. –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏ –¥–∞–Ω–Ω—ã–µ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–ª–∏—Å—å —Å –ø–æ–º–æ—â—å—é @st.cache –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–∞. –≠—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –ø—Ä–∏ –≤–Ω–µ—Å–µ–Ω–∏–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ. –≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å —Å–æ—Ç–Ω–∏ —Ä–∞–∑, –µ—Å–ª–∏ —Ç—ã –∞–∫—Ç–∏–≤–Ω–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ—à—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ! –í –≤–µ—Ä—Å–∏–∏ 0.89.0 Streamlit –∑–∞–ø—É—Å—Ç–∏–ª –¥–≤–∞ –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–∏—Ç–∏–≤–∞ (st.experimental_memo –∏ st.experimental_singleton), —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—Å–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å @st.cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–∞–π–ø–ª–∞–π–Ω –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è\n",
    "1. –°–æ–∑–¥–∞—ë—Ç—Å—è –∏ –∑–∞–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ñ–∞–π–ª `app.py` (default, –º–æ–∂–µ—Ç–µ —Å–≤–æ–π)\n",
    "2. `streamlit run app.py`\n",
    "3. Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. –¢–µ–∫—Å—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.title(\"This is a title\")\n",
    "st.header(\"This is a header\")\n",
    "st.subheader(\"This is a subheader\")\n",
    "st.text(\"This is a text\")\n",
    "st.markdown(\"# This is a markdown header 1\")\n",
    "st.markdown(\"## This is a markdown header 2\")\n",
    "st.markdown(\"### This is a markdown header 3\")\n",
    "st.markdown(\"This is a markdown: *bold* **italic** `inline code` ~strikethrough~\")\n",
    "st.markdown(\"\"\"This is a code block with syntax highlighting\n",
    "```python\n",
    "print(\"Hello world!\")\n",
    "```\n",
    "\"\"\")\n",
    "st.html(\n",
    "    \"image from url example with html: \"\n",
    "    \"<img src='https://www.wallpaperflare.com/static/450/825/286/kitten-cute-animals-grass-5k-wallpaper.jpg' width=400px>\",\n",
    ")\n",
    "\n",
    "\n",
    "st.write(\"Text with write\")\n",
    "st.write(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.success(\"Success\")\n",
    "st.info(\"Information\")\n",
    "st.warning(\"Warning\")\n",
    "st.error(\"Error\")\n",
    "exp = ZeroDivisionError(\"Trying to divide by Zero\")\n",
    "st.exception(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. –û–±—ä–µ–∫—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "request.urlretrieve(\n",
    "    \"http://craphound.com/images/1006884_2adf8fc7.jpg\",\n",
    "    \"image_example.jpg\",\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "img = Image.open(\"image_example.jpg\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –∫–∞—Ä—Ç–∏–Ω–∫–∞ (–±–µ–∑ html - –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π)\n",
    "st.image(img, width=200)\n",
    "\n",
    "# —á–µ–∫–±–æ–∫—Å\n",
    "if st.checkbox(\"Show/Hide\"):\n",
    "    st.text(\"Showing the widget\")\n",
    "else:\n",
    "    st.warning(\"Not showing what is inside\")\n",
    "\n",
    "# –≤—ã–±–æ—Ä –æ–ø—Ü–∏–∏ –∫—Ä—É–∂–æ—á–∫–∞–º–∏\n",
    "status = st.radio(\"Select Gender: \", ('Male', 'Female'))\n",
    "if (status == 'Male'):\n",
    "    st.success(\"Male\")\n",
    "else:\n",
    "    st.success(\"Female\")\n",
    "\n",
    "# –≤—ã–±–æ—Ä –æ–ø—Ü–∏–∏ –≤—ã–ø–∞–¥–∞—é—â–∏–º –º–µ–Ω—é\n",
    "hobby = st.selectbox(\n",
    "    \"Hobbies: \",\n",
    "    ['Dancing', 'Reading', 'Sports'],\n",
    ")\n",
    "st.write(\"Your hobby is: \", hobby)\n",
    "\n",
    "# –≤—ã–±–æ—Ä –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–ø—Ü–∏–π\n",
    "hobbies = st.multiselect(\n",
    "    \"Hobbies: \",\n",
    "    ['Dancing', 'Reading', 'Sports'],\n",
    ")\n",
    "st.write(\"You selected\", len(hobbies), 'hobbies')\n",
    "\n",
    "# –∫–Ω–æ–ø–∫–∞ –±–µ–∑ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞\n",
    "st.button(\"Click me for no reason\")\n",
    "\n",
    "# –∫–Ω–æ–ø–∫–∞, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è —Ç–µ–∫—Å—Ç, –∫–æ–≥–¥–∞ –Ω–∞–∂–∞—Ç–∞\n",
    "if(st.button(\"Click me\")):\n",
    "    st.text(\"You did it, you clicked me!!!\")\n",
    "\n",
    "# —Ç–µ–∫—Å—Ç–æ–≤—ã–π input: label - –Ω–∞–∑–≤–∞–Ω–∏–µ, value - —á—Ç–æ –Ω–∞–ø–∏—Å–∞–Ω–æ –ø–æ –¥–µ—Ñ–æ–ª—Ç—É\n",
    "name = st.text_input(label=\"Enter Your name\", value=\"Type Here ...\")\n",
    "if(st.button('Submit')):\n",
    "    result = name.title()\n",
    "    st.success(result)\n",
    "\n",
    "# —Å–ª–∞–π–¥–µ—Ä\n",
    "level = st.slider(\"Select the level\", 1, 5)\n",
    "st.text('Selected: {}'.format(level))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. –°–ª–æ–∂–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–±—â–∞—è –Ω–∞ rerun - —Å–ø–æ—Å–æ–± —à–µ–π—Ä–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –º–µ–∂–¥—É –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏\n",
    "st.session_state  # kinda Dict[str, Any]\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "if 'key' not in st.session_state:\n",
    "    st.session_state['key'] = 'value'\n",
    "\n",
    "# –ú–æ–∂–Ω–æ —Ç–∞–∫–∂–µ –æ–±—Ä–∞—â–∞—Ç—å—Å—è –ø–æ –∞—Ç—Ä–∏–±—É—Ç–∞–º, –∞ –Ω–µ –∫–ª—é—á–∞–º\n",
    "if 'key' not in st.session_state:\n",
    "    st.session_state.key = 'value'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ\n",
    "st.session_state.key1 = 'value1'     # Attribute API\n",
    "st.session_state['key2'] = 'value2'  # Dictionary like API\n",
    "\n",
    "# –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —á—Ç–æ –≤ st.session_state\n",
    "st.write(st.session_state)\n",
    "\n",
    "# magic\n",
    "st.session_state\n",
    "\n",
    "# –æ—à–∏–±–∫–∞ –µ—Å–ª–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫–ª—é—á\n",
    "st.write(st.session_state['missing_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key - –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–∫–∞–∑–∞—Ç—å –≤ –∫–∞–∫–æ–µ –ø–æ–ª–µ session_state –∑–∞–ø–∏—Å–∞—Ç—å –æ–±—ä–µ–∫—Ç\n",
    "st.text_input(\"Please input something\", key=\"my input\")\n",
    "st.session_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –µ—Å—Ç—å 2 –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–∞\n",
    "\n",
    "```python\n",
    "@st.cache_data      # –¥–ª—è –¥–∞–Ω–Ω—ã—Ö - —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã—Ö–æ–¥–æ–≤ —Å –∫–ª—é—á–∞–º–∏ –≤—Ö–æ–¥–æ–≤\n",
    "@st.cache_resource  # –¥–ª—è –º–æ–¥–µ–ª–µ–π / —Ä–µ—Å—É—Ä—Å–æ–≤ - –Ω–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–µ –æ–±—ä–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Ö–æ—á–µ—Ç—Å—è –∑–∞–≥—Ä—É–∂–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "@st.cache_data  # –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "def load_data(url):\n",
    "    df = pd.read_csv(url)  # —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    return df\n",
    "\n",
    "df = load_data(\"https://github.com/plotly/datasets/raw/master/uber-rides-data1.csv\")\n",
    "st.dataframe(df)\n",
    "\n",
    "st.button(\"Rerun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "@st.cache_resource  # –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "def load_model():\n",
    "    return pipeline(\"sentiment-analysis\")  # —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "query = st.text_input(\"Your query\", value=\"I love Streamlit! üéà\")\n",
    "if query:\n",
    "    result = model(query)[0]  # –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º\n",
    "    st.write(query)\n",
    "    st.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HF + StreamLit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–Ω–æ –ø–æ–¥–Ω—è—Ç—å —Ç–µ—Å—Ç–æ–≤—É—é streamlit api –ø—Ä—è–º–æ –Ω–∞ hugging face\n",
    "\n",
    "1. https://huggingface.co/\n",
    "2. New space - Streamlit\n",
    "3. –î–µ–ª–∞–µ–º `app.py` –∏ `requirements.txt`\n",
    "4. –°–æ–±–∏—Ä–∞–µ—Ç—Å—è –¥–æ–∫–µ—Ä –æ–±—Ä–∞–∑ - –ø–æ—è–≤–ª—è–µ—Ç—Å—è app (–ø—É–±–ª–∏—á–Ω–æ –¥–æ—Å—Ç—É–ø–µ–Ω)\n",
    "5. \\* –Ω–µ–º–Ω–æ–≥–æ —Ö—É–ª–∏–≥–∞–Ω—Å—Ç–≤–∞ - –º–æ–∂–Ω–æ –¥–æ—Å—Ç–∞—Ç—å –¥–∞–∂–µ iframe –∏–∑ hf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
