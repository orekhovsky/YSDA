{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d43fbba",
   "metadata": {
    "cellId": "6tgifddr0g4wmt8cy6lx7h"
   },
   "source": [
    "# **Seminar 4 - Интерпретация Нейросетей**\n",
    "*Naumov Anton (Any0019)*\n",
    "\n",
    "*To contact me in telegram: @any0019*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d08d4a",
   "metadata": {
    "cellId": "l5w7dxjwy9qp98m1xhmr",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Почему мы любим PyTorch - Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba17b28",
   "metadata": {
    "cellId": "hvdz8lt3ofr1005sp1s6j"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6ac4a",
   "metadata": {
    "cellId": "b1ijaau7orjka9ftbza88"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dca736",
   "metadata": {
    "cellId": "f9jjd2z227v9h6m63vmuyv"
   },
   "outputs": [],
   "source": [
    "nn.Module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a74d0f",
   "metadata": {
    "cellId": "4jeetepwf0pxdy76c9ved",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn.Module??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62935346",
   "metadata": {
    "cellId": "on5wugrb2u7a0xbq0581v",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.1. Простой модуль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602f833",
   "metadata": {
    "cellId": "ovl50zog3yhq02zanfzyo"
   },
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # Задаём через nn.Parameter, чтобы torch знал, что это обучаемые веса модуля\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return (input @ self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe129ce",
   "metadata": {
    "cellId": "mz5r3t4w83ddcr9ql36qx"
   },
   "outputs": [],
   "source": [
    "m = MyLinear(4, 3)\n",
    "sample_input = torch.randn(4)\n",
    "print(sample_input, m(sample_input), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f0bbf",
   "metadata": {
    "cellId": "3vern0sfsbmmme211whzj"
   },
   "outputs": [],
   "source": [
    "for name, param in m.named_parameters():\n",
    "    print(f\"Name ~ '{name}'\", param, sep=\"\\n\", end=\"\\n-----\\n\")\n",
    "\n",
    "# # Аналогично, но только сами параметры, когда не нужны имена\n",
    "# for param in m.parameters():\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ccb71",
   "metadata": {
    "cellId": "5ct89zwcvosomrgpdu4po"
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "  MyLinear(4, 3),\n",
    "  nn.ReLU(),\n",
    "  MyLinear(3, 1)\n",
    ")\n",
    "\n",
    "sample_input = torch.randn(4)\n",
    "print(sample_input, net(sample_input), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c7bbb3",
   "metadata": {
    "cellId": "b7cdrfjagtc73bv09ggw3m",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.2. Нейросеть с SubModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893770e1",
   "metadata": {
    "cellId": "m9aysh7wdlkzylm4go16vp"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l0 = MyLinear(4, 3)\n",
    "        self.l1 = MyLinear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1631622a",
   "metadata": {
    "cellId": "65d60lw0r6cxn4sj810rj"
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "for name, child in net.named_children():\n",
    "    print(f\"Name ~ '{name}'\", child, child.parameters(), sep=\"\\n\", end=\"\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce29602b",
   "metadata": {
    "cellId": "xcs1i6hwhs9skl372hvqa",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.3. Сложная нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede97900",
   "metadata": {
    "cellId": "s0a1dpp7aeb2ugeuas6ve",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BigNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = MyLinear(5, 4)\n",
    "        self.net = Net()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(self.l1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769d6f81",
   "metadata": {
    "cellId": "4meswezl58xhv264s9f8l"
   },
   "outputs": [],
   "source": [
    "# pip install termcolor -- если не установлен пакет\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8a0c8",
   "metadata": {
    "cellId": "wivfs9m1kmwcnrog4d4v",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "big_net = BigNet()\n",
    "\n",
    "\n",
    "print(colored(\"Children:\\n\", color=\"red\", attrs=[\"bold\", \"underline\"]))\n",
    "for name, child in big_net.named_children():\n",
    "    print(f\"Name ~ '{name}'\", child, sep=\"\\n\", end=\"\\n-----\\n\")\n",
    "print(\"\\n========\\n\")\n",
    "\n",
    "\n",
    "print(colored(\"Modules:\\n\", color=\"red\", attrs=[\"bold\", \"underline\"]))\n",
    "for name, module in big_net.named_modules():\n",
    "    print(f\"Name ~ '{name}'\", module, sep=\"\\n\", end=\"\\n-----\\n\")\n",
    "print(\"\\n========\\n\")\n",
    "\n",
    "\n",
    "print(colored(\"Parameters:\\n\", color=\"red\", attrs=[\"bold\", \"underline\"]))\n",
    "for name, param in big_net.named_parameters():\n",
    "    print(f\"Name ~ '{name}'\", param, sep=\"\\n\", end=\"\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8f59a",
   "metadata": {
    "cellId": "temjuow8ns5u5obl9ls",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.4. Динамические модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f441be4",
   "metadata": {
    "cellId": "huik6g0bqhspx9qogjxepn"
   },
   "outputs": [],
   "source": [
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super().__init__()\n",
    "        # nn.ModuleList - список модулей\n",
    "        self.linears = nn.ModuleList(\n",
    "            [MyLinear(4, 4) for _ in range(num_layers)]\n",
    "        )\n",
    "        # nn.ModuleDict - словарь модулей\n",
    "        self.activations = nn.ModuleDict({\n",
    "          \"relu\": nn.ReLU(),\n",
    "          \"lrelu\": nn.LeakyReLU()\n",
    "        })\n",
    "        self.final = MyLinear(4, 1)\n",
    "\n",
    "    def forward(self, x, act):\n",
    "        for linear in self.linears:\n",
    "            x = linear(x)\n",
    "        x = self.activations[act](x)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "dynamic_net = DynamicNet(3)\n",
    "sample_input = torch.randn(4)\n",
    "output = dynamic_net(sample_input, \"relu\")\n",
    "print(sample_input, output, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2ac890",
   "metadata": {
    "cellId": "ari5uhjtlcmckh0cnlld",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.5. Состояние модуля (train vs eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47991bc4",
   "metadata": {
    "cellId": "h0ufc2l73bpbtaocxad1fi"
   },
   "outputs": [],
   "source": [
    "class ModalModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # Добавляет константу, но только в .train() режиме\n",
    "            return x + 1.\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b57d1",
   "metadata": {
    "cellId": "o6hhpwcznmjtzbc3jxy99"
   },
   "outputs": [],
   "source": [
    "m = ModalModule()\n",
    "x = torch.randn(4)\n",
    "\n",
    "print(f\"Input:\\n{x}\\n\")\n",
    "\n",
    "print(f\"Training mode output:\\n{m(x)}\\n\")\n",
    "\n",
    "m.eval()\n",
    "print(f\"Evaluation mode output:\\n{m(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c1184",
   "metadata": {
    "cellId": "m453m9ejer5lpf2n8rxi",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.6. Тип данных и вычислений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f427cfb0",
   "metadata": {
    "cellId": "jns265fggpxhco9ftm7ob"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "dtype = torch.float64\n",
    "\n",
    "# Переместить все параметры модели на device\n",
    "dynamic_net.to(device=device)\n",
    "# dynamic_net.cpu()\n",
    "# dynamic_net.cuda(int: ...)\n",
    "\n",
    "# Изменить тип всех параметров модели\n",
    "dynamic_net.to(dtype=dtype)\n",
    "# dynamic_net = dynamic_net.double()  # float64\n",
    "# dynamic_net = dynamic_net.float()  # float32\n",
    "# dynamic_net = dynamic_net.half() # float16\n",
    "\n",
    "sample_input = sample_input.to(device, dtype=dtype)\n",
    "\n",
    "output = dynamic_net(sample_input, \"relu\")\n",
    "\n",
    "print(sample_input, output, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d88013",
   "metadata": {
    "cellId": "ul91677dgsltlgcre4muq",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.7. Применение функций к модулям нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c512df",
   "metadata": {
    "cellId": "kkjq5djnl6xhvnb0oo3rf"
   },
   "outputs": [],
   "source": [
    "# Сделаем функцию для инициализации весов модели\n",
    "# обёртка no_grad() - используется тут, чтобы избежать подсчёта градиентов для этой операции\n",
    "@torch.no_grad()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight, gain=1.0)  # см следующую ячейку\n",
    "        m.bias.fill_(0.0)\n",
    "\n",
    "# Применяем функцию рекурсивно ко всем модулям и подмодулям\n",
    "dynamic_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1600122",
   "metadata": {
    "cellId": "1hkzv4inuwa59afw3nkma5"
   },
   "source": [
    "Xavier / Glorot initialization\n",
    "\n",
    "`Understanding the difficulty of training deep feedforward neural networks` - Glorot, X. & Bengio, Y. (2010) ([ссылка](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf))\n",
    "\n",
    "$$\\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{n}_{in} + \\text{n}_{out}}}$$\n",
    "\n",
    ", где $n_{in}$ и $n_{out}$ - число входов и выходов слоя соответственно\n",
    "\n",
    "Веса получаются следующим образом: $w \\sim \\mathcal{N}(0, std^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45195d0",
   "metadata": {
    "cellId": "ig2cces2h0c4iz4iucswl4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.8. Сохранение и загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125fe04b",
   "metadata": {
    "cellId": "0bxgsr02g6j2pmmc0xz6f7"
   },
   "outputs": [],
   "source": [
    "big_net = BigNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89908d40",
   "metadata": {
    "cellId": "81boht5siurulh6584ubnm"
   },
   "outputs": [],
   "source": [
    "# Словарь со всеми весами модели\n",
    "big_net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777b80c",
   "metadata": {
    "cellId": "aqt99rmdjqbntai58e0y6"
   },
   "outputs": [],
   "source": [
    "# Сохраняем state_dict нашей модели\n",
    "torch.save(\n",
    "    big_net.state_dict(),\n",
    "    \"net.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f2ebd1",
   "metadata": {
    "cellId": "215dowlsxnp3kh5gg0ajhd"
   },
   "outputs": [],
   "source": [
    "# Инициализируем модель с таким же набором параметров\n",
    "new_big_net = BigNet()\n",
    "\n",
    "# Загружаем state_dict сохранённой модели в память\n",
    "state_dict = torch.load(\"net.pt\")\n",
    "print(state_dict)\n",
    "\n",
    "# Подгружаем state_dict в инициализированную модель\n",
    "new_big_net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f7904",
   "metadata": {
    "cellId": "b11cyq2n2bk3u440kax7g"
   },
   "source": [
    "**[Важно!] В общем случае - сохраняйте всю необходимую информацию о модели, оптимайзере, этапе обучения, ...**\n",
    "\n",
    "Best practice $\\longrightarrow$ подумайте, что вам будет нужно, если вы захотите, загрузив назад модель, продолжить её обучение:\n",
    "* обучаемые параметры\n",
    "* оптимизатор\n",
    "* шедулер\n",
    "* какие графики рисуете\n",
    "* на какой эпохе находитесь\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d6ee60",
   "metadata": {
    "cellId": "3oytsj45hc8ewx1n782vs"
   },
   "outputs": [],
   "source": [
    "# К примеру:\n",
    "\n",
    "# torch.save(\n",
    "#     {\n",
    "#         \"epoch\": epoch,\n",
    "#         \"model_state_dict\": model.state_dict(),\n",
    "#         \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "#         \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "#         \"losses\": losses,\n",
    "#     },\n",
    "#     chkp_path,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca58a00",
   "metadata": {
    "cellId": "4l322ztbi3x0r24z3i52x2",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.9. Буфферы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4b2202",
   "metadata": {
    "cellId": "5pbf2ybdetb1ahpomjuomti"
   },
   "outputs": [],
   "source": [
    "nn.Module.register_buffer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ca2e5",
   "metadata": {
    "cellId": "1mraymrxqyybq3ufquzi8"
   },
   "outputs": [],
   "source": [
    "class RunningMean(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        # регистрируем буфер - параметр модели, но не обучаемый\n",
    "        self.register_buffer(\n",
    "            \"mean\",\n",
    "            torch.zeros(num_features),\n",
    "            persistent=True,  # содержится ли в state_dict модели?\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n",
    "        return self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e4ac2",
   "metadata": {
    "cellId": "qo7mjzw14v9c5on27b4jj"
   },
   "outputs": [],
   "source": [
    "m = RunningMean(4)\n",
    "for _ in range(10):\n",
    "    input = torch.randn(4)\n",
    "    m(input)\n",
    "\n",
    "print(m.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020acb2d",
   "metadata": {
    "cellId": "et16dtdrqbqsd6yoyn5hj"
   },
   "outputs": [],
   "source": [
    "print(colored(\"Parameters:\\n\", color=\"red\", attrs=[\"bold\", \"underline\"]))\n",
    "for name, param in m.named_parameters():\n",
    "    print(f\"Name ~ '{name}'\", param, sep=\"\\n\", end=\"\\n-----\\n\")\n",
    "print(\"\\n========\\n\")\n",
    "\n",
    "\n",
    "print(colored(\"Buffers:\\n\", color=\"red\", attrs=[\"bold\", \"underline\"]))\n",
    "for name, buffer in m.named_buffers():\n",
    "    print(f\"Name ~ '{name}'\", buffer, sep=\"\\n\", end=\"\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ff830a",
   "metadata": {
    "cellId": "4ndizgv95ee5votyvlkmss",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.10. Инициализация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6b2cdc",
   "metadata": {
    "cellId": "9suw6do1f4guwucn0ueapn"
   },
   "source": [
    "Все параметры и floating point буфферы инициализируются на этапе инициализации модуля:\n",
    "* Тип ~ `param.float()` или `param.to(dtype=torch.float32)`.\n",
    "* Устройство ~ `param.cpu()`, или `param.to(\"cpu\")`, или `param.to(torch.device(\"cpu\"))`.\n",
    "* Инициализация значений ~ схема, соответствующая исторически предпочитаемой инициализацией для данного вида слоёв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e43e3",
   "metadata": {
    "cellId": "q01tykyu6od3oy2pev87y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Инициализировать на другом устройстве сразу\n",
    "m = nn.Linear(5, 3, device='cuda')\n",
    "\n",
    "# Инициализировать другим типом данных сразу\n",
    "m = nn.Linear(5, 3, dtype=torch.half)\n",
    "\n",
    "# Пропустить стандартную инициализацию и провести кастомную (для примера ортогональную)\n",
    "m = nn.Linear(5, 3)\n",
    "nn.init.orthogonal_(m.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4289b3",
   "metadata": {
    "cellId": "ik2s13mmc2hu04kliaz6q",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Forward / Backward hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bc098",
   "metadata": {
    "cellId": "4aa7d4ota2p3zog5im4zyh"
   },
   "source": [
    "Hooks - способ взаимодействия с `torch.Tensor` и/или `nn.Module` для получения и модификации входов / выходов / градиентов в момент их прохода через forward / backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1116707c",
   "metadata": {
    "cellId": "ax511kucdw43nzylktgbfj",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.1. Module level hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548a7da",
   "metadata": {
    "cellId": "hvy27qbmpsegzemglzztgs"
   },
   "outputs": [],
   "source": [
    "def forward_pre_hook(m, inputs):\n",
    "    # Исполняется перед выполнением forward на соответствующем элементе\n",
    "    # Может изменить входы в forward\n",
    "    print(\n",
    "        colored(\"Froward pre hook\", color=\"red\", attrs=[\"bold\", \"underline\"]),\n",
    "        inputs,\n",
    "        \" - Sizes ~ [\" + \", \".join([str(el.shape) for el in inputs if el is not None]) + \"]\",\n",
    "        sep=\"\\n\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "    return None  # None - не менять inputs, либо new_inputs\n",
    "\n",
    "def forward_hook(m, inputs, output):\n",
    "    # Исполняется после выполнения forward на соответствующем элементе\n",
    "    # Может изменить выход из forward\n",
    "    print(\n",
    "        colored(\"Froward hook\", color=\"red\", attrs=[\"bold\", \"underline\"]),\n",
    "        inputs,\n",
    "        \" - Sizes ~ [\" + \", \".join([str(el.shape) for el in inputs if el is not None]) + \"]\",\n",
    "        output,\n",
    "        f\" - Size ~ {output.shape}\",\n",
    "        sep=\"\\n\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "    return None  # None - не менять output, либо new_output\n",
    "\n",
    "def backward_hook(m, grad_inputs, grad_outputs):\n",
    "    # Исполняется после выполнения backward на соответствующем элементе\n",
    "    # Может изменить grad_inputs (выход на этапе backward)\n",
    "    print(\n",
    "        colored(\"Backward hook\", color=\"red\", attrs=[\"bold\", \"underline\"]),\n",
    "        grad_inputs,\n",
    "        \" - Sizes ~ [\" + \", \".join([str(el.shape) for el in grad_inputs if el is not None]) + \"]\",\n",
    "        grad_outputs,\n",
    "        \" - Sizes ~ [\" + \", \".join([str(el.shape) for el in grad_outputs if el is not None]) + \"]\",\n",
    "        sep=\"\\n\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "    return None  # None - не менять grad_inputs, либо new_grad_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82960c",
   "metadata": {
    "cellId": "80ml9tn5yf2jeg80arnxd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = nn.Linear(4, 1)\n",
    "\n",
    "fp_handle = m.register_forward_pre_hook(forward_pre_hook)\n",
    "f_handle = m.register_forward_hook(forward_hook)\n",
    "# b_handle = m.register_backward_hook(backward_hook)  # --> deprecated\n",
    "b_handle = m.register_full_backward_hook(backward_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cdd306",
   "metadata": {
    "cellId": "re8jopt8ivhj2028agyf"
   },
   "outputs": [],
   "source": [
    "sample_input = torch.randn(3, 4)\n",
    "sample_input.requires_grad = True\n",
    "\n",
    "print(\"Input\", sample_input, sep=\"\\n\", end=\"\\n\\n\")\n",
    "\n",
    "out = m(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74edc21e",
   "metadata": {
    "cellId": "pw77676by9iplhn0fl5ngb"
   },
   "outputs": [],
   "source": [
    "out.backward(torch.ones_like(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585b2505",
   "metadata": {
    "cellId": "t9g9kf4crlqbiehbmwoae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fp_handle.remove()\n",
    "f_handle.remove()\n",
    "b_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54724acd",
   "metadata": {
    "cellId": "5egwcjcbc0v3l71x1moebm"
   },
   "outputs": [],
   "source": [
    "out = m(sample_input)\n",
    "out.backward(torch.ones_like(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13511f",
   "metadata": {
    "cellId": "vm77vj22ogc3yi6eids56",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.2. Tensor level hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fddbd59",
   "metadata": {
    "cellId": "c8q9ql4gse4iog277696u"
   },
   "outputs": [],
   "source": [
    "def tensor_hook(grad):\n",
    "    print(\n",
    "        colored(\"Tensor backward hook\", color=\"red\", attrs=[\"bold\", \"underline\"]),\n",
    "        grad,\n",
    "        f\" - Size ~ {grad.shape}\",\n",
    "        sep=\"\\n\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "    return None  # None - не менять grad, либо new_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7cc7b0",
   "metadata": {
    "cellId": "mx4v7mojyacdqtpct6yrue",
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = nn.Linear(4, 1)\n",
    "\n",
    "w_t_handle = m.weight.register_hook(tensor_hook)\n",
    "b_t_handle = m.bias.register_hook(tensor_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112424e2",
   "metadata": {
    "cellId": "glyki87smfnoqefva8lgp"
   },
   "outputs": [],
   "source": [
    "sample_input = torch.randn(3, 4)\n",
    "sample_input.requires_grad = True\n",
    "\n",
    "print(\"Input\", sample_input, sep=\"\\n\", end=\"\\n\\n\")\n",
    "\n",
    "out = m(sample_input)\n",
    "out.backward(torch.ones_like(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfddf80c",
   "metadata": {
    "cellId": "oqbcnz4sqykl9wxtdeha5"
   },
   "outputs": [],
   "source": [
    "m.bias.grad, m.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583599b2",
   "metadata": {
    "cellId": "o4fs425bzj6fgnwijyblk"
   },
   "outputs": [],
   "source": [
    "w_t_handle.remove()\n",
    "b_t_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee236810",
   "metadata": {
    "cellId": "nodfs0h6byoz70kffh8c"
   },
   "outputs": [],
   "source": [
    "out = m(sample_input)\n",
    "out.backward(torch.ones_like(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2743371",
   "metadata": {
    "cellId": "afhq5928ak35blpalf5ih",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.3. Как достать и сохранить что-то из модели?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d69021f",
   "metadata": {
    "cellId": "yd7vwyhqztli6chonahvj"
   },
   "source": [
    "```python\n",
    "from collections import defaultdict\n",
    "\n",
    "hook_data = defaultdict(list)\n",
    "\n",
    "def hook(*args):\n",
    "    # берём из global scope-а перменную, созданную раньше\n",
    "    global hook_data\n",
    "    \n",
    "    ...\n",
    "    for key, value in ...:\n",
    "        hook_data[key].append(value)\n",
    "    \n",
    "    return None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696bd3fd",
   "metadata": {
    "cellId": "tpebeirzklnx3lb32ibr5n",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Интерпретация нейросетей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7bc03",
   "metadata": {
    "cellId": "xet08a7tqbpoj5e3f6u22r"
   },
   "source": [
    "Главный вопрос интерпретации - **почему нейросеть повела себя так, как повела?**\n",
    "\n",
    "* Почему ответ был именно такой в конкретном случае?\n",
    "* Что нужно подать на вход, чтобы получить подобный ответ?\n",
    "* На что сильнее всего смотрит нейросеть, принимая решение?\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9d9e0",
   "metadata": {
    "cellId": "sc2p7e905bgil8mol7q05k"
   },
   "source": [
    "Все стандартные подходы интерпретации из классического ML так же будут работать и с нейросетями (рассматривая нейросеть как функцию от входов как отдельных переменных), но при этом многие из них часто ориентированы на рассмотрение важности одной или нескольких фичей, а в случае с нейросетями часто входы будут иметь тысячи или даже миллионы фичей на входе (к пр., картинка 1920 x 1080 x 3 пикселей), что выйдет не очень хорошо:\n",
    "* значимость одной фичи маленькая\n",
    "* виды данных имеют свои особенности и зависимости\n",
    "* виды моделей имеют свои подходы к их анализу\n",
    "\n",
    "Сегодня мы будем рассматривать подходы для анализа нейросетей (на примере свёрточных нейросетей), подходы глобально можно разделить на группы по нескольким признакам:\n",
    "1. Анализ данных и зоны видимости при активации нейросетей\n",
    "2. Attribution - изучаем какая часть входа(ов) отвечает за активацию нейросети\n",
    "3. Feature visualization - подбираем изображения, наиболее соответствующие ожиданиям нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa08b0cf",
   "metadata": {
    "cellId": "qozee9gnttg1rwhbmryeos",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.1. Подготовим модель и данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956097d",
   "metadata": {
    "cellId": "mbzyiamevf9kt2peixxl8"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade torchvision==0.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283513ad",
   "metadata": {
    "cellId": "yw6e14qwwiew9iw5gt9znh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!:bash\n",
    "python3 --version\n",
    "python3 -c \"import torchvision; import torch; print(f'torch ~ {torch.__version__}\\ntorchvision ~ {torchvision.__version__}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf91ac",
   "metadata": {
    "cellId": "f9to2hazsr87h3a1lu3hl4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from importlib import reload\n",
    "reload(torch)\n",
    "reload(torchvision)\n",
    "print(torch.__version__, torchvision.__version__, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a7cab",
   "metadata": {
    "cellId": "h4m0bo0crcvzqui88hapk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca25b409",
   "metadata": {
    "cellId": "au8fhz1xpguq4swidjxi8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = \"../Sem3 - DL tricks/data\"\n",
    "original_train_ds = datasets.STL10(root=dataset_path, split=\"train\", download=True)\n",
    "original_val_ds = datasets.STL10(root=dataset_path, split=\"test\", download=True)\n",
    "classes = original_train_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41719656",
   "metadata": {
    "cellId": "fjdf49gcfj0wadgy18cdxl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "h = 4\n",
    "w = 8\n",
    "fig, ax = plt.subplots(h, w, figsize=(30, 15))\n",
    "\n",
    "fig.suptitle(f\"all classes ~ [{', '.join(classes)}]\", y=0.85 + 0.02*h)\n",
    "for i in range(h * w):\n",
    "    plt.subplot(h, w, i+1)\n",
    "    img, cl = original_train_ds[i]\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(f\"{cl} ~ {classes[cl]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833565bb",
   "metadata": {
    "cellId": "a8n5p3i6anctg1katmyntn",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torch import nn\n",
    "\n",
    "def get_model_and_transforms():\n",
    "    weights = models.ResNet18_Weights.DEFAULT\n",
    "    model = models.resnet18(weights=weights, progress=True)\n",
    "\n",
    "    # замораживаем первые слои\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(\"layer4\"):\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # заменяем классификатор\n",
    "    model.fc = nn.Linear(model.fc.in_features, len(classes))\n",
    "\n",
    "    transforms = weights.transforms()\n",
    "    \n",
    "    return model, transforms\n",
    "\n",
    "model, transforms = get_model_and_transforms()\n",
    "\n",
    "print(model, transforms, sep=\"\\n=========\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741b452",
   "metadata": {
    "cellId": "23922ul551akvuyjrf5w",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from termcolor import colored\n",
    "\n",
    "def beautiful_int(i):\n",
    "    i = str(i)\n",
    "    return \".\".join(reversed([i[max(j, 0):j+3] for j in range(len(i) - 3, -3, -3)]))\n",
    "\n",
    "# Counting how many parameters does our model have\n",
    "def model_num_params(model, verbose_all=True, verbose_only_learnable=False):\n",
    "    sum_params = 0\n",
    "    sum_learnable_params = 0\n",
    "    for param in model.named_parameters():\n",
    "        num_params = np.prod(param[1].shape)\n",
    "        if verbose_all or (verbose_only_learnable and param[1].requires_grad):\n",
    "            print(\n",
    "                colored(\n",
    "                    '{: <42} ~  {: <9} params ~ grad: {}'.format(\n",
    "                        param[0],\n",
    "                        beautiful_int(num_params),\n",
    "                        param[1].requires_grad,\n",
    "                    ),\n",
    "                    {True: \"green\", False: \"red\"}[param[1].requires_grad],\n",
    "                )\n",
    "            )\n",
    "        sum_params += num_params\n",
    "        if param[1].requires_grad:\n",
    "            sum_learnable_params += num_params\n",
    "    print(\n",
    "        f'\\nIn total:\\n  - {beautiful_int(sum_params)} params\\n  - {beautiful_int(sum_learnable_params)} learnable params'\n",
    "    )\n",
    "    return sum_params, sum_learnable_params\n",
    "\n",
    "\n",
    "sum_params, sum_learnable_params = model_num_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8eb09c",
   "metadata": {
    "cellId": "38ok2p0hlyti1rb5c1hs6f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms as tr\n",
    "\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = tr.Compose([\n",
    "    tr.Resize(size=(256, 256)),\n",
    "    tr.RandomRotation(degrees=(-10, 10)),\n",
    "    tr.RandomCrop(size=(224, 224)),\n",
    "    tr.RandomHorizontalFlip(),\n",
    "    tr.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 0.5)),\n",
    "    tr.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    tr.ToTensor(),\n",
    "    tr.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "val_transform = torchvision.transforms.Compose([\n",
    "    tr.Resize(size=(224, 224)),\n",
    "    tr.ToTensor(),\n",
    "    tr.Normalize(mean=mean, std=std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4aad68",
   "metadata": {
    "cellId": "vy9ndyi9z2ov2138tbciec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def de_normalize(img):\n",
    "    img = img.detach().numpy().transpose((1, 2, 0))\n",
    "    return img * std + mean\n",
    "\n",
    "\n",
    "img_ind = 0\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 7))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(original_train_ds[img_ind][0])\n",
    "plt.title(\"Before\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(de_normalize(train_transform(original_train_ds[img_ind][0])))\n",
    "plt.title(\"After train transform\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(de_normalize(val_transform(original_train_ds[img_ind][0])))\n",
    "plt.title(\"After val transform\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed643fc",
   "metadata": {
    "cellId": "k8cm20d0vp6u5shce4xyj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataset, transforms):\n",
    "        super(ImageDataset).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, cl = self.dataset[index]\n",
    "        return self.transforms(img), cl\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5d1ff",
   "metadata": {
    "cellId": "8tby4fygnm8rrdlihmf97",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = ImageDataset(original_train_ds, train_transform)\n",
    "val_ds = ImageDataset(original_val_ds, val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4027a-3ae8-4b4c-b1cb-8b04d32c47fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(train_ds), len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ae736",
   "metadata": {
    "cellId": "lprqppkx9sabj159c17do",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6459efd7",
   "metadata": {
    "cellId": "pob9xuigppfq3s9js1ter",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.2. Дообучим модельку на задачу классификации STL10 датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd479d",
   "metadata": {
    "cellId": "ocm6v5t9j8sh5dzjl2m88e"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def create_model_and_optimizer(lr=1e-3, beta1=0.9, beta2=0.999, device=\"cpu\"):\n",
    "    model, _ = get_model_and_transforms()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    params = []\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            params.append(param)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params, lr, [beta1, beta2])\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader, criterion):\n",
    "    model.train()\n",
    "    losses_tr = []\n",
    "    for images, targets in tqdm(loader):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(images)\n",
    "        loss = criterion(out, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses_tr.append(loss.item()) \n",
    "    \n",
    "    return model, optimizer, np.mean(losses_tr)\n",
    "\n",
    "\n",
    "def val(model, loader, criterion, metric_names=None):\n",
    "    model.eval()\n",
    "    losses_val = []\n",
    "    if metric_names:\n",
    "        metrics = {name: [] for name in metric_names}\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(loader):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            out = model(images)\n",
    "            loss = criterion(out, targets)\n",
    "            losses_val.append(loss.item())\n",
    "            \n",
    "            if metric_names:\n",
    "                if 'accuracy' in metrics:\n",
    "                    _, pred_classes = torch.max(out, dim=-1)\n",
    "                    metrics['accuracy'].append((pred_classes == targets).float().mean().item())\n",
    "                if 'top2accuracy' in metrics:\n",
    "                    preds = torch.argsort(out, dim=1, descending=True)\n",
    "                    metrics['top2accuracy'].append(\n",
    "                        np.mean([targets[i] in preds[i, :2] for i in range(len(targets))])\n",
    "                    )\n",
    "                if 'top3accuracy' in metrics:\n",
    "                    preds = torch.argsort(out, dim=1, descending=True)\n",
    "                    metrics['top3accuracy'].append(\n",
    "                        np.mean([targets[i] in preds[i, :3] for i in range(len(targets))])\n",
    "                    )\n",
    "    \n",
    "        if metric_names:\n",
    "            for name in metrics:\n",
    "                metrics[name] = np.mean(metrics[name])\n",
    "    \n",
    "    return np.mean(losses_val), metrics if metric_names else None\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "\n",
    "def learning_loop(model, optimizer, train_loader, val_loader, criterion, scheduler=None, min_lr=None, epochs=10, val_every=1, draw_every=1, metric_names=None):\n",
    "    losses = {'train': [], 'val': []}\n",
    "    lrs = []\n",
    "    if metric_names:\n",
    "        metrics = {name: [] for name in metric_names}\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f'#{epoch}/{epochs}:')\n",
    "        model, optimizer, loss = train(model, optimizer, train_loader, criterion)\n",
    "        losses['train'].append(loss)\n",
    "\n",
    "        if not (epoch % val_every):\n",
    "            loss, metrics_ = val(model, val_loader, criterion, metric_names)\n",
    "            losses['val'].append(loss)\n",
    "            if metric_names:\n",
    "                for name in metrics_:\n",
    "                    metrics[name].append(metrics_[name])\n",
    "            \n",
    "            lrs.append(get_lr(optimizer))\n",
    "            if scheduler:\n",
    "                try:\n",
    "                    scheduler.step()\n",
    "                except:\n",
    "                    scheduler.step(loss)\n",
    "\n",
    "        if not (epoch % draw_every):\n",
    "            clear_output(True)\n",
    "            ww = 3 if metric_names else 2\n",
    "            fig, ax = plt.subplots(1, ww, figsize=(20, 10))\n",
    "            fig.suptitle(f'#{epoch}/{epochs}:')\n",
    "\n",
    "            plt.subplot(1, ww, 1)\n",
    "            plt.title('losses')\n",
    "            plt.plot(losses['train'], 'r.-', label='train')\n",
    "            plt.plot(losses['val'], 'g.-', label='val')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, ww, 2)\n",
    "            plt.title('learning rate')\n",
    "            plt.plot(lrs, '.-', label='lr')\n",
    "            plt.legend()\n",
    "            \n",
    "            if metric_names:\n",
    "                plt.subplot(1, ww, 3)\n",
    "                plt.title('additional metrics')\n",
    "                for name in metric_names:\n",
    "                    plt.plot(metrics[name], '.-', label=name)\n",
    "                plt.legend()\n",
    "            \n",
    "            plt.show()\n",
    "        \n",
    "        if min_lr and get_lr(optimizer) <= min_lr:\n",
    "            print(f'Learning process ended with early stop after epoch {epoch}')\n",
    "            break\n",
    "    \n",
    "    return model, optimizer, losses, lrs, metrics if metric_names else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3adcd0e",
   "metadata": {
    "cellId": "44v3i8spl9vn5dkieezy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model, optimizer = create_model_and_optimizer(\n",
    "    lr = 1e-4,\n",
    "    device = device,\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model, optimizer, losses, lrs, metrics = learning_loop(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    criterion = criterion,\n",
    "    scheduler = scheduler,\n",
    "    epochs = NUM_EPOCHS,\n",
    "    min_lr = None,\n",
    "    metric_names = {'accuracy', 'top3accuracy'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd215f56",
   "metadata": {
    "cellId": "004jpbtz696xa4y904wdtzwc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "chkp_path = \"./model.pt\"\n",
    "\n",
    "# Save\n",
    "torch.save(\n",
    "    {\n",
    "        'epoch': NUM_EPOCHS,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'losses': losses,\n",
    "    },\n",
    "    chkp_path,\n",
    ")\n",
    "\n",
    "\n",
    "# Load\n",
    "# checkpoint = torch.load(chkp_path)\n",
    "\n",
    "# model, optimizer = create_model_and_optimizer(\n",
    "#     lr = 1e-4,\n",
    "#     device = device,\n",
    "# )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# losses = checkpoint['losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a877fac1",
   "metadata": {
    "cellId": "i298eowszrjzt8y4gxnj1k"
   },
   "outputs": [],
   "source": [
    "def real_confusion_matrix(model, val_loader, class_labels, use_probs=False, normalize=True, round_size=4):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_classes = len(class_labels)\n",
    "        conf_matrix = np.zeros((n_classes, n_classes))\n",
    "        for i, (img, cl) in enumerate(tqdm(val_loader)):\n",
    "            probs = model(img.to(device)).exp()\n",
    "            if use_probs:\n",
    "                for j in range(img.shape[0]):\n",
    "                    for c in range(n_classes):\n",
    "                        conf_matrix[cl[j].item(), c] += probs[j,c]\n",
    "            else:\n",
    "                _, pred_classes = torch.max(probs, 1)\n",
    "                for j in range(img.shape[0]):\n",
    "                    conf_matrix[cl[j].item(), pred_classes[j].item()] += 1.\n",
    "        \n",
    "        if normalize:\n",
    "            conf_matrix /= conf_matrix.sum(1)\n",
    "        \n",
    "        fig = plt.figure(figsize=(18, 10))\n",
    "        fig.suptitle(f'Confusion matrix (norm={normalize}, use_probs={use_probs})')\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(conf_matrix.T)\n",
    "        fig.colorbar(cax)\n",
    "        \n",
    "        @plt.FuncFormatter\n",
    "        def fake_labels(x, pos):\n",
    "            return class_labels[(int(x))] if x < len(class_labels) else \"@\"\n",
    "        \n",
    "        ax.xaxis.set_ticks(list(range(len(class_labels))))\n",
    "        ax.xaxis.set_ticklabels(class_labels)\n",
    "        ax.set_xlabel('predicted class')\n",
    "        \n",
    "        ax.yaxis.set_ticks(list(range(len(class_labels))))\n",
    "        ax.yaxis.set_ticklabels(class_labels)\n",
    "        ax.set_ylabel('true class')\n",
    "        \n",
    "        for x in range(conf_matrix.shape[0]):\n",
    "            for y in range(conf_matrix.shape[1]):\n",
    "                ax.text(x, y, round(conf_matrix[x,y], round_size), va='center', ha='center')\n",
    "        \n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cecc6df",
   "metadata": {
    "cellId": "ogmxqfz36s7q6aoa9h90j"
   },
   "outputs": [],
   "source": [
    "pcm = real_confusion_matrix(\n",
    "    model,\n",
    "    val_loader,\n",
    "    classes,\n",
    "    use_probs=True,\n",
    "    normalize=True,\n",
    "    round_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b3016",
   "metadata": {
    "cellId": "5dqltn6o2547ifj673ng6n"
   },
   "outputs": [],
   "source": [
    "pcm = real_confusion_matrix(\n",
    "    model,\n",
    "    val_loader,\n",
    "    classes,\n",
    "    use_probs=False,\n",
    "    normalize=True,\n",
    "    round_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f48b5",
   "metadata": {
    "cellId": "w4i7atq6zi1uw2h5ljcr9",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.3. Что видит каждый слой нейросети?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a62810e",
   "metadata": {
    "cellId": "8t0orwro835m0bu11lu2"
   },
   "source": [
    "Посмотрим как *примерно* выглядят входы каждого соответствующего слоя нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8038cb53",
   "metadata": {
    "cellId": "vhz7qhsw0nqw26994ut7a"
   },
   "outputs": [],
   "source": [
    "# # Убрать hooks без handle:\n",
    "# from collections import OrderedDict\n",
    "# from typing import Dict, Callable\n",
    "\n",
    "# module._forward_hooks: Dict[int, Callable] = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042f667",
   "metadata": {
    "cellId": "zq746n48dvl29dx46cj3aq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_inputs = []\n",
    "handles = []\n",
    "\n",
    "def extract_input_pre_hook(m, inputs):\n",
    "    global layer_inputs\n",
    "    layer_inputs.append([m, inputs[0]])\n",
    "    return None\n",
    "\n",
    "conv_layers = []\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        if \"downsample\" not in name:\n",
    "            conv_layers.append(module)\n",
    "            handles.append(module.register_forward_pre_hook(extract_input_pre_hook))\n",
    "    elif isinstance(module, (nn.MaxPool2d, nn.AvgPool2d)):\n",
    "        conv_layers.append(module)\n",
    "        handles.append(module.register_forward_pre_hook(extract_input_pre_hook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a049db",
   "metadata": {
    "cellId": "1sho9ws50oelrarwzhkzz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3425ac",
   "metadata": {
    "cellId": "1ig72r3zh27gzksfmco7cj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def min_max_scale(img):\n",
    "    img = img - img.min()\n",
    "    return img / img.max()\n",
    "\n",
    "img_ind = 4\n",
    "\n",
    "img = val_ds[img_ind][0]\n",
    "plt.imshow(min_max_scale(de_normalize(img)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec79fb",
   "metadata": {
    "cellId": "sciuu0oy52kdpwt2yjmnp9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_hw(s):\n",
    "    h = int(np.round(np.sqrt(s)))\n",
    "    w = s // h + (s % h > 0)\n",
    "    return h, w\n",
    "\n",
    "h, w = get_hw(conv_layers[0].weight.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(35, 35))\n",
    "fig.suptitle(\"Layer 1 kernels\", y=0.9)\n",
    "for i, kernel in enumerate(conv_layers[0].weight):\n",
    "    plt.subplot(h, w, i + 1)\n",
    "    plt.imshow(\n",
    "        min_max_scale(\n",
    "            kernel.detach().numpy().transpose(1, 2, 0)\n",
    "        )\n",
    "    )\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108dcc7",
   "metadata": {
    "cellId": "u6c3m2zvs5o70r990g0eb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "out = model(img[None, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341cb004",
   "metadata": {
    "cellId": "wfpwum3g7pkxtd72kuzeeo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for handle in handles:\n",
    "    handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e315983",
   "metadata": {
    "cellId": "2d6bm7lauepqu1oihcyvvs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    fmaps = [conv_layers[0](img.unsqueeze(0))]\n",
    "    for module in conv_layers[1:]:\n",
    "        fmaps.append(module(fmaps[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c13024",
   "metadata": {
    "cellId": "ptz9w35o2bcy884tdp1to",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_layer_fmap(fmaps, layer_ind, show_first=None, h=None):\n",
    "    actual = isinstance(fmaps[layer_ind], list)\n",
    "    if actual:\n",
    "        fmap = fmaps[layer_ind+1][1][0]\n",
    "    else:\n",
    "        fmap = fmaps[layer_ind][0]\n",
    "    s = show_first or fmap.shape[0]\n",
    "    \n",
    "    if h is None:\n",
    "        h, w = get_hw(s)\n",
    "    else:\n",
    "        w = s // h + (s % h > 0)\n",
    "    fig, ax = plt.subplots(h, w, figsize=(w * 5, h * 5))\n",
    "    fig.suptitle(f\"Layer #{layer_ind} {'actual' if actual else 'approximate'} feature maps\", y=0.9)\n",
    "    i = 1\n",
    "    for fmap_img in fmap:\n",
    "        if show_first and i > show_first:\n",
    "            break\n",
    "        if fmap_img.sum() == 0:\n",
    "            continue\n",
    "        plt.subplot(h, w, i)\n",
    "        plt.imshow(\n",
    "            min_max_scale(fmap_img.detach()),\n",
    "            cmap=\"gray\",\n",
    "        )\n",
    "        plt.axis('off')\n",
    "        i += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b178db86",
   "metadata": {
    "cellId": "abxn7gktp7ie7bpfrixz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_layer_fmap(fmaps, 0, show_first=10, h=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a862f",
   "metadata": {
    "cellId": "nrbkbrnlunsdfo9nlqugld",
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_layer_fmap(layer_inputs, 0, show_first=10, h=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e246c7e3",
   "metadata": {
    "cellId": "wl3nos2yv7a8gqnll3oduh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_layer_fmap(fmaps, 5, show_first=10, h=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47e5b66",
   "metadata": {
    "cellId": "1wr06v37vsb83hwrwfr7mu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_layer_fmap(layer_inputs, 5, show_first=10, h=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959448c",
   "metadata": {
    "cellId": "7t4mtiwon3kg7kfccmdskv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_layer_fmap(fmaps, 10, show_first=10, h=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d8251",
   "metadata": {
    "cellId": "3fr6g9vo254lkj6nhyk8l",
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_layer_fmap(layer_inputs, 10, show_first=10, h=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820f5eb",
   "metadata": {
    "cellId": "cav80sl4ptb90nzz9im8mq"
   },
   "source": [
    "Не слишком информативно, хотя и даёт понимание о сложности интерпретации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221f7294",
   "metadata": {
    "cellId": "mgi9e8z66al5ihn2j7sdms",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.4. Анализ относительно датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40af368",
   "metadata": {
    "cellId": "ijvxqry4ue88rnwmct7rt2",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.4.1. Картинки, сильнее всего активирующиеся на фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e65a6",
   "metadata": {
    "cellId": "65bbj98zrpj9sppprv40dg"
   },
   "outputs": [],
   "source": [
    "val_preds = []\n",
    "val_conv_activations = []\n",
    "val_pre_fc_preds = []\n",
    "targets = []\n",
    "\n",
    "def extract_input_hook(m, inputs, output):\n",
    "    val_conv_activations.append(inputs[0].cpu().detach().numpy())\n",
    "    val_pre_fc_preds.append(output.cpu().detach().numpy())\n",
    "    return None\n",
    "\n",
    "handle = model.avgpool.register_forward_hook(extract_input_hook)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    for img, cl in tqdm(val_loader):\n",
    "        out = model(img.to(device))\n",
    "        val_preds.append(out.cpu().detach().numpy())\n",
    "        targets.append(cl.cpu().detach().numpy())\n",
    "model.cpu()\n",
    "\n",
    "val_preds = np.concatenate(val_preds, axis=0)\n",
    "val_pre_fc_preds = np.concatenate(val_pre_fc_preds, axis=0)\n",
    "val_conv_activations = np.concatenate(val_conv_activations, axis=0)\n",
    "targets = np.concatenate(targets, axis=0)\n",
    "\n",
    "handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc7fef",
   "metadata": {
    "cellId": "igp7jqzzxtsdtr7pw1bc"
   },
   "outputs": [],
   "source": [
    "val_preds.shape, val_pre_fc_preds.shape, val_conv_activations.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5691711",
   "metadata": {
    "cellId": "z6smogg0fvusfs0sei1oe"
   },
   "outputs": [],
   "source": [
    "cl_to_ind = {cl: i for i, cl in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382eae58",
   "metadata": {
    "cellId": "n6lmq394uijzrj0h6d29g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_class = \"cat\"\n",
    "ind = cl_to_ind[selected_class]\n",
    "\n",
    "mask = (targets == ind).astype(bool)\n",
    "pre_fc_weights = model.fc.weight.detach().numpy()[ind, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99274b9c",
   "metadata": {
    "cellId": "qzetsdi76dh9yqrf4jcpb"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25, 5))\n",
    "plt.plot(pre_fc_weights)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0016e975",
   "metadata": {
    "cellId": "r7cagj85sn8i0p11updjc"
   },
   "outputs": [],
   "source": [
    "select_top = 4\n",
    "best_features = np.argsort(-pre_fc_weights)\n",
    "worst_features = best_features[-select_top:]\n",
    "best_features = best_features[:select_top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d37f44",
   "metadata": {
    "cellId": "dmdj0peunqs23p3151fung"
   },
   "outputs": [],
   "source": [
    "show_top = 5\n",
    "\n",
    "fig, ax = plt.subplots(select_top * 2, show_top, figsize=(25, 10 * select_top))\n",
    "\n",
    "for i, feature_ind in enumerate(best_features):\n",
    "    best_img_inds = np.argsort(-val_pre_fc_preds[:, feature_ind, 0, 0])[:show_top]\n",
    "    for j, img_ind in enumerate(best_img_inds):\n",
    "        plt.subplot(select_top * 2, show_top, i * show_top + j + 1)\n",
    "        plt.imshow(original_val_ds[img_ind][0])\n",
    "        plt.title(f\"Top #{i+1} best feature\\nTop #{j+1} best image\")\n",
    "\n",
    "for i, feature_ind in enumerate(worst_features):\n",
    "    worst_img_inds = np.argsort(-val_pre_fc_preds[:, feature_ind, 0, 0])[:show_top]\n",
    "    for j, img_ind in enumerate(worst_img_inds):\n",
    "        plt.subplot(select_top * 2, show_top, (select_top + i) * show_top + j + 1)\n",
    "        plt.imshow(original_val_ds[img_ind][0])\n",
    "        plt.title(f\"Top #{select_top-i} worst feature\\nTop #{j+1} best image\")\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6062473",
   "metadata": {
    "cellId": "made68rjg3nw0vy21xq0h",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.4.2. Как посчитать receptive_field нейрона на каком-то слое?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14fa2f",
   "metadata": {
    "cellId": "44vponmmosoqe0x00xb8bm"
   },
   "source": [
    "Картинка для понимания:\n",
    "\n",
    "![receptive_field](receptive_field.jpg \"Receptive Field\")\n",
    "\n",
    "Посчитаем относительно только одной из размерностей (H или W), для второй аналогично.\n",
    "\n",
    "$$k_t - \\text{Kernel size on layer t}$$\n",
    "$$s_t - \\text{Stride on layer t}$$\n",
    "$$d_t - \\text{Dilation on layer t}$$\n",
    "$$r_t - \\text{Receptive field of neuron on layer t}$$\n",
    "\n",
    "1. $r_0 = 1$ - на 0-м слое каждый \"нейрон\" (пиксель) хранит в себе информацию только о самом себе\n",
    "2. $r_1 = k_1$ - на 1-м слое каждый нейрон смотрит ровно на $k_1$ пикселей\n",
    "3. $r_2 = (k_2 - 1) * s_1 + k_1$ - на 1-м слое один нейрон смотрит на $k_1$, а каждый следующий сдвинут от него на $s_1$, таких следующих нейронов $k_2 - 1$ штук, поэтому получаем такую формулу для 2-го слоя\n",
    "4. Как перейти к общему случаю? $r_{t+1} = (k_{t+1} - 1) \\cdot j_t + r_t \\longrightarrow$ на t-ом слое один нейрон смотрит на $r_t$, а каждый следующий смещён от него на jump $j_t$ пикселей оригинального изображения, где $j_t = \\prod_{i=1}^{t} s_i$\n",
    "5. Как здесь участвует dilation? $r_{t+1} = ((k_{t+1} - 1) \\cdot d_{t+1} + 1 - 1) \\cdot j_t + r_t = (k_{t+1} - 1) \\cdot d_{t+1} \\cdot j_t + r_t\\longrightarrow$ фактически увеличивает kernel size $k_{t}^* = (k_{t} - 1) \\cdot d_{t} + 1$\n",
    "\n",
    "**Итоговоая формула:**\n",
    "$$ r_{t+1} = (k_{t+1} - 1) \\cdot d_{t+1} \\cdot j_t + r_t = \\sum_{i=1}^{t+1} \\Big( (k_{i} - 1) \\cdot d_{i} \\cdot \\prod_{j=1}^{i} s_j \\Big) + 1$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc33f0",
   "metadata": {
    "cellId": "cuwdmu8bxbpdk49nu57re"
   },
   "outputs": [],
   "source": [
    "conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98967d",
   "metadata": {
    "cellId": "smjxf0ab84dxyx8mlx28y"
   },
   "outputs": [],
   "source": [
    "def get_receptive_fields(conv_layers):\n",
    "    def receptive_field(old_receptive_field, jump, kernel_size):\n",
    "        return old_receptive_field + (kernel_size - 1) * jump\n",
    "    \n",
    "    res = [1]\n",
    "    old_receptive_field = 1\n",
    "    jump = 1\n",
    "    for layer in conv_layers:\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            k = layer.kernel_size[0]\n",
    "            d = layer.dilation[0]\n",
    "            s = layer.stride[0]\n",
    "        elif isinstance(layer, (nn.MaxPool2d, nn.AvgPool2d)):\n",
    "            k = layer.kernel_size\n",
    "            d = layer.dilation\n",
    "            s = layer.stride\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown layer type {type(layer)}\")\n",
    "        \n",
    "        old_receptive_field = receptive_field(\n",
    "            old_receptive_field,\n",
    "            jump,\n",
    "            (k - 1) * d + 1,\n",
    "        )\n",
    "        jump *= s\n",
    "        res.append(old_receptive_field)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "receptive_fields = get_receptive_fields(conv_layers)\n",
    "\n",
    "for i, (layer_, from_, to_) in enumerate(zip(conv_layers, receptive_fields[:-1], receptive_fields[1:])):\n",
    "    layer_type = str(type(layer_)).split(\".\")[-1][:-2]\n",
    "    k = layer_.kernel_size\n",
    "    s = layer_.stride\n",
    "    d = layer_.dilation\n",
    "    if isinstance(k, tuple):\n",
    "        k = k[0]\n",
    "        s = s[0]\n",
    "        d = d[0]\n",
    "    print(f\"#{i: <2}: {layer_type: <9} - k={k} , s={s} , d={d}  ~  changed receptive field  ~  {from_: >3} --> {to_: <3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22601b3b",
   "metadata": {
    "cellId": "ygtfajczyltbp3dbd4qidw",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.4.3. Посмотрим на какие части изображений активируются какие слои"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0727ab5",
   "metadata": {
    "cellId": "ueiz73dq5aq9pioh0vry7b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "chosen_layers = [3, 6, 9, 12]\n",
    "max_num_channels = 10\n",
    "\n",
    "layer_outputs = defaultdict(list)\n",
    "\n",
    "def extract_output_hook(m, inputs, output, layer_ind):\n",
    "    layer_outputs[layer_ind].append(output[:, :max_num_channels, :, :].cpu().detach().numpy())\n",
    "    return None\n",
    "\n",
    "handles = []\n",
    "\n",
    "for i, module in enumerate(conv_layers):\n",
    "    if i in chosen_layers:\n",
    "        assert module._forward_hooks == OrderedDict(), f\"Delete previous hooks first\\n{i}\\n{module}\\n{module._forward_hooks}\"\n",
    "        handles.append(\n",
    "            module.register_forward_hook(\n",
    "                partial(extract_output_hook, layer_ind=i)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09ea00",
   "metadata": {
    "cellId": "ui4vx9lmxnvigmh368vwl"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    for img, _ in tqdm(val_loader):\n",
    "        out = model(img.to(device))\n",
    "model.cpu()\n",
    "\n",
    "for key, values in layer_outputs.items():\n",
    "    layer_outputs[key] = np.concatenate(values, axis=0)\n",
    "\n",
    "for handle in handles:\n",
    "    handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39020b26",
   "metadata": {
    "cellId": "kipwavh4lwi5qlj4uibh"
   },
   "outputs": [],
   "source": [
    "layer_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdebf9a",
   "metadata": {
    "cellId": "aia72phwxx4z9fdmkq4qv"
   },
   "outputs": [],
   "source": [
    "layer_outputs[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed139fcd",
   "metadata": {
    "cellId": "g7n7ku6wvz9rwkiu8amto"
   },
   "outputs": [],
   "source": [
    "# # Force clear everything up\n",
    "# for module in conv_layers:\n",
    "#     module._forward_hooks = OrderedDict()\n",
    "\n",
    "# del layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e7248",
   "metadata": {
    "cellId": "on1eq16e22dlzpdlccqt3j"
   },
   "outputs": [],
   "source": [
    "pad_size = [0]\n",
    "j = 1\n",
    "\n",
    "for module in conv_layers:\n",
    "    p = module.padding\n",
    "    s = module.stride\n",
    "    if isinstance(p, tuple):\n",
    "        p = p[0]\n",
    "        s = s[0]\n",
    "    pad_size.append(pad_size[-1] + p * j)\n",
    "    j *= s\n",
    "    \n",
    "pad_size = pad_size[1:]\n",
    "print(pad_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b47299",
   "metadata": {
    "cellId": "v5309tnlyvn3h1yhwy8kuz"
   },
   "outputs": [],
   "source": [
    "def find_best_examples(layer_ind, num_channels=5, num_examples=5):\n",
    "    assert num_channels <= max_num_channels, \"This amount of channels wasn't computed\"\n",
    "    res = np.zeros((num_channels, num_examples, 3), dtype=int)\n",
    "    \n",
    "    p = pad_size[layer_ind]\n",
    "    r = receptive_fields[layer_ind + 1]\n",
    "    h, w = layer_outputs[layer_ind].shape[2:]\n",
    "    for i, channel_ind in enumerate(range(num_channels)):\n",
    "        max_values = np.max(\n",
    "            layer_outputs[layer_ind][:, channel_ind, :, :],\n",
    "            axis=(1, 2),\n",
    "        )\n",
    "        \n",
    "        best_images_inds = np.argsort(-max_values)[:num_examples]\n",
    "        \n",
    "        for j, image_ind in enumerate(best_images_inds):\n",
    "            # На каком изображении из val датасета\n",
    "            res[i, j, 0] = image_ind\n",
    "            # Положение максимальной активации по осям H и W (с учётом смещённости за счёт паддинга)\n",
    "            pos = np.argmax(layer_outputs[layer_ind][image_ind, channel_ind].reshape(-1), axis=0)\n",
    "            res[i, j, 1] = pos // w - p\n",
    "            res[i, j, 2] = pos % w - p\n",
    "            \n",
    "            assert (\n",
    "                np.max(layer_outputs[layer_ind][image_ind, channel_ind]) ==\n",
    "                layer_outputs[layer_ind][image_ind, channel_ind, res[i, j, 1] + p, res[i, j, 2] + p]\n",
    "            ), (\n",
    "                f\"\\n{layer_outputs[layer_ind][image_ind, channel_ind, res[i, j, 1], res[i, j, 2]]}\"\n",
    "                f\"\\n{np.max(layer_outputs[layer_ind][image_ind, channel_ind])}\"\n",
    "            )\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(num_channels, num_examples, figsize=(5 * num_examples, 5 * num_channels))\n",
    "    fig.suptitle(f\"Layer #{layer_ind} activations examples\", y=0.9)\n",
    "    \n",
    "    for i, channel_ind in enumerate(range(num_channels)):\n",
    "        for j, image_ind in enumerate(range(num_examples)):\n",
    "            plt.subplot(num_channels, num_examples, i * num_examples + j + 1)\n",
    "            img = np.clip(de_normalize(val_ds[res[channel_ind, image_ind, 0]][0]), 0, 1)\n",
    "            hh, ww = res[channel_ind, image_ind, 1:]\n",
    "            img = img[max(hh, 0): hh + r, max(ww, 0): ww + r, :]\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Channel #{i+1}\\nImage #{j+1} with top activation\")\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d48f41",
   "metadata": {
    "cellId": "s83hbct7r5ilmrddpgnfq"
   },
   "outputs": [],
   "source": [
    "find_best_examples(layer_ind=3, num_channels=10, num_examples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bfc7d8",
   "metadata": {
    "cellId": "0d4zjmr87tn0pw0ma4py3dc"
   },
   "outputs": [],
   "source": [
    "find_best_examples(layer_ind=6, num_channels=10, num_examples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaba851",
   "metadata": {
    "cellId": "8nev98laxzpgn5ag8s2o6"
   },
   "outputs": [],
   "source": [
    "find_best_examples(layer_ind=9, num_channels=10, num_examples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979f741",
   "metadata": {
    "cellId": "os3ggxxzz2zzrcdhyp0sl"
   },
   "outputs": [],
   "source": [
    "find_best_examples(layer_ind=12, num_channels=10, num_examples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bee3ca",
   "metadata": {
    "cellId": "9hleq9q6xdolo1dpo3429"
   },
   "outputs": [],
   "source": [
    "del layer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915d3db",
   "metadata": {
    "cellId": "uisbedokkzq9rjro4n9vj",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.4.4. Какая часть изображения ответственна за результат классификации?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad2c681",
   "metadata": {
    "cellId": "keuny81xunf0d4x6g7k3bd"
   },
   "outputs": [],
   "source": [
    "img_ind = 4\n",
    "\n",
    "img = val_ds[img_ind][0]\n",
    "img_vis = min_max_scale(de_normalize(img))\n",
    "plt.imshow(img_vis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb0c3f",
   "metadata": {
    "cellId": "u1ijgwuiwdbee76qd77d9e"
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(img.unsqueeze(0))[0].detach()\n",
    "    probs = F.softmax(out, dim=0).numpy()\n",
    "\n",
    "pred_cl_ind = np.argmax(probs)\n",
    "\n",
    "print(\"\\n\".join(list(map(lambda el: f\"{el[0]: <10} ~ {str(round(el[1], 4))}\", zip(classes, probs)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9825ab",
   "metadata": {
    "cellId": "r993ma7odflihpju7bw6ef",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "assert np.allclose(\n",
    "    probs,\n",
    "    F.softmax(torch.tensor(val_preds[img_ind]), dim=0).numpy(),\n",
    ")\n",
    "\n",
    "relevance = model.fc.weight[pred_cl_ind].detach().numpy() * val_pre_fc_preds[img_ind, :, 0, 0]\n",
    "\n",
    "# class activation map\n",
    "cam = (val_conv_activations[img_ind] * relevance[:, None, None]).sum(0)\n",
    "\n",
    "cam = cv2.resize(cam, val_transform.transforms[0].size)\n",
    "\n",
    "cam = min_max_scale(cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb616401",
   "metadata": {
    "cellId": "1ys5yg4hv9nurl4s25p6m"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(img_vis)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title(\"Class Activation Map\")\n",
    "plt.imshow(cam)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title(\"Both\")\n",
    "plt.imshow(img_vis, alpha=0.5)\n",
    "plt.imshow(cam, alpha=0.5)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c8427",
   "metadata": {
    "cellId": "kvjv517opnflvv55dgzhq"
   },
   "outputs": [],
   "source": [
    "def get_cam(img_ind):\n",
    "    img = val_ds[img_ind][0]\n",
    "    img = min_max_scale(de_normalize(img))\n",
    "    \n",
    "    probs = F.softmax(torch.tensor(val_preds[img_ind]), dim=0).numpy()\n",
    "    \n",
    "    pred_cl_ind = np.argmax(probs)\n",
    "    \n",
    "    to_print = list(map(lambda el: f\"{el[0]: <10} ~ {str(round(el[1], 4))}\", zip(classes, probs)))\n",
    "    to_print[targets[img_ind]] = colored(to_print[targets[img_ind]], \"green\")\n",
    "    to_print[pred_cl_ind] = colored(to_print[pred_cl_ind], \"red\")  # , attrs=[\"bold\", \"underline\"])\n",
    "    print(\"\\n\".join(to_print))\n",
    "    \n",
    "    relevance = model.fc.weight[pred_cl_ind].detach().numpy() * val_pre_fc_preds[img_ind, :, 0, 0]\n",
    "\n",
    "    # class activation map\n",
    "    cam = (val_conv_activations[img_ind] * relevance[:, None, None]).sum(0)\n",
    "\n",
    "    cam = cv2.resize(cam, val_transform.transforms[0].size)\n",
    "\n",
    "    cam = min_max_scale(cam)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.title(\"Image\")\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.title(\"Class Activation Map\")\n",
    "    plt.imshow(cam)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.title(\"Both\")\n",
    "    plt.imshow(img, alpha=0.5)\n",
    "    plt.imshow(cam, alpha=0.5)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168426aa",
   "metadata": {
    "cellId": "miuky8uocfthpx40k2pov6"
   },
   "outputs": [],
   "source": [
    "get_cam(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94f4d19",
   "metadata": {
    "cellId": "0j4ztvwp77i51qyj4r6rclb"
   },
   "outputs": [],
   "source": [
    "get_cam(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8f30c",
   "metadata": {
    "cellId": "4y20a5kzmrl6hafcg1t0yv"
   },
   "outputs": [],
   "source": [
    "get_cam(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d4666",
   "metadata": {
    "cellId": "wbfhyn28a5bjh43aweit6b"
   },
   "outputs": [],
   "source": [
    "get_cam(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c2eae3",
   "metadata": {
    "cellId": "rmgmbdmp2mohuoj1kppx"
   },
   "outputs": [],
   "source": [
    "val_probs = np.exp(val_preds - val_preds.max()) / np.exp(val_preds - val_preds.max()).sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caef18c",
   "metadata": {
    "cellId": "ml9cq61wm3gzivtljnwf2"
   },
   "outputs": [],
   "source": [
    "least_configdent = np.argsort(val_probs.max(1))[:5]\n",
    "least_configdent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee171e",
   "metadata": {
    "cellId": "z198wnclnaa0hxyl2zyg489"
   },
   "outputs": [],
   "source": [
    "get_cam(least_configdent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b0b207",
   "metadata": {
    "cellId": "u4exdwn7f0hy44ju04use"
   },
   "outputs": [],
   "source": [
    "get_cam(least_configdent[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c1bb2",
   "metadata": {
    "cellId": "8hntbyao6m6alxnh4dvulk"
   },
   "outputs": [],
   "source": [
    "get_cam(least_configdent[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142637b8",
   "metadata": {
    "cellId": "tr63wf12759xa27c3l3bqe"
   },
   "outputs": [],
   "source": [
    "get_cam(least_configdent[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082ba2a",
   "metadata": {
    "cellId": "4czssd3zttzugztty8ztwn",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.5. Как посмотреть что ищет слой нейросети, не по данным, а по самой модели?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26621ed4",
   "metadata": {
    "cellId": "od18fx78okhs81v5ib1p"
   },
   "source": [
    "**Feature Visualization by Optimization:**\n",
    "\n",
    "Формально говоря, модель так же дефиренцируема по своим входам, так почему бы нам не \"обучить\" идеальный вход для конкретного нейрона / канала / слоя / выхода / ...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d71bb88",
   "metadata": {
    "cellId": "5tqde14bs06ukezfkam5ia"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57affb4d",
   "metadata": {
    "cellId": "bwc8d49zwynxsdpzmovss",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.5.1. Представление класса (Gradient based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08731d1d",
   "metadata": {
    "cellId": "s8dk7vd9qbhf2ott0q9a"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "max_generation_epochs = 4000\n",
    "alpha = 2.\n",
    "alpha_reduce_each = 500\n",
    "alpha_reduce_mul = 0.5\n",
    "draw_each = 100\n",
    "chosen_class = \"cat\"\n",
    "\n",
    "class_ind = cl_to_ind[chosen_class]\n",
    "img_shape = val_ds[0][0].shape\n",
    "synthetic_image = torch.nn.Parameter(torch.rand(len(classes), *img_shape), requires_grad=True)\n",
    "starting_image = synthetic_image[class_ind].detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "generated_probs = defaultdict(list)\n",
    "optimized_values = defaultdict(list)\n",
    "xx = np.arange(len(classes))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "synthetic_image = synthetic_image.to(device)\n",
    "synthetic_image.retain_grad()\n",
    "for epoch in trange(max_generation_epochs):\n",
    "    synthetic_image.grad = None\n",
    "    out = model(synthetic_image)\n",
    "    pred_class_logit = out[xx, xx]\n",
    "    pred_class_logit.sum().backward()\n",
    "    synthetic_image.data = synthetic_image.data + alpha * synthetic_image.grad\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probs = F.softmax(out, dim=1)\n",
    "        pred_class_prob = probs[xx, xx]\n",
    "        for ind in xx:\n",
    "            generated_probs[ind].append(pred_class_prob[ind].item())\n",
    "            optimized_values[ind].append(pred_class_logit[ind].item())\n",
    "        if (epoch + 1) % draw_each == 0:\n",
    "            clear_output(True)\n",
    "            fig = plt.subplots(2, 1, figsize=(20, 8))\n",
    "            \n",
    "            plt.subplot(211)\n",
    "            for ind in xx:\n",
    "                plt.plot(generated_probs[ind])\n",
    "            plt.title(\"Probability of target class\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.ylabel(\"probability\")\n",
    "            plt.yscale(\"log\")\n",
    "            \n",
    "            plt.subplot(212)\n",
    "            for ind in xx:\n",
    "                plt.plot(optimized_values[ind])\n",
    "            plt.title(\"Logit of target class\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            \n",
    "            plt.show()\n",
    "        if (epoch + 1) % alpha_reduce_each == 0:\n",
    "            alpha *= alpha_reduce_mul\n",
    "\n",
    "model.cpu()\n",
    "synthetic_image = synthetic_image.cpu()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(starting_image)\n",
    "plt.title(\"Starting image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(min_max_scale(synthetic_image[class_ind].detach().numpy().transpose(1, 2, 0)))\n",
    "plt.title(\"Resulting image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e296d3",
   "metadata": {
    "cellId": "7iclmyhrjwenqg8shm23e"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(25, 12))\n",
    "\n",
    "for ind in range(10):\n",
    "    plt.subplot(2, 5, ind+1)\n",
    "    plt.imshow(min_max_scale(synthetic_image[ind].detach().numpy().transpose(1, 2, 0)))\n",
    "    plt.title(classes[ind])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb4dd2",
   "metadata": {
    "cellId": "ctf6z54o08dayn2z55lypl"
   },
   "source": [
    "Не особенно на что-то похоже...\n",
    "\n",
    "Потому что мы никак не ограничиваем изображение на то, чтобы оно было хоть сколько-то похоже на изображение в нашем понимании.\n",
    "\n",
    "Добавим член, отвечающий за L2-регуляризацию изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d95107",
   "metadata": {
    "cellId": "b3fm4sythtfvkigf0dffmr"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "max_generation_epochs = 4000\n",
    "alpha = 4.\n",
    "alpha_reduce_each = 500\n",
    "alpha_reduce_mul = 0.5\n",
    "draw_each = 100\n",
    "chosen_class = \"cat\"\n",
    "l2_coef = 4.\n",
    "\n",
    "class_ind = cl_to_ind[chosen_class]\n",
    "img_shape = val_ds[0][0].shape\n",
    "synthetic_image = torch.nn.Parameter(torch.rand(len(classes), *img_shape), requires_grad=True)\n",
    "starting_image = synthetic_image[class_ind].detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "generated_probs = defaultdict(list)\n",
    "optimized_values = defaultdict(list)\n",
    "xx = np.arange(len(classes))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "synthetic_image = synthetic_image.to(device)\n",
    "synthetic_image.retain_grad()\n",
    "for epoch in trange(max_generation_epochs):\n",
    "    synthetic_image.grad = None\n",
    "    out = model(synthetic_image)\n",
    "    pred_class_logit = out[xx, xx]\n",
    "    l2_norm = (synthetic_image**2).sum(dim=(1,2,3)).sqrt().sum(0)\n",
    "    (pred_class_logit.sum() - l2_coef * l2_norm).backward()\n",
    "    synthetic_image.data = synthetic_image.data + alpha * synthetic_image.grad\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probs = F.softmax(out, dim=1)\n",
    "        pred_class_prob = probs[xx, xx]\n",
    "        for ind in xx:\n",
    "            generated_probs[ind].append(pred_class_prob[ind].item())\n",
    "            optimized_values[ind].append(pred_class_logit[ind].item())\n",
    "        if (epoch + 1) % draw_each == 0:\n",
    "            clear_output(True)\n",
    "            fig = plt.subplots(2, 1, figsize=(20, 8))\n",
    "            \n",
    "            plt.subplot(211)\n",
    "            for ind in xx:\n",
    "                plt.plot(generated_probs[ind])\n",
    "            plt.title(\"Probability of target class\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.ylabel(\"probability\")\n",
    "            plt.yscale(\"log\")\n",
    "            \n",
    "            plt.subplot(212)\n",
    "            for ind in xx:\n",
    "                plt.plot(optimized_values[ind])\n",
    "            plt.title(\"Logit of target class\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            \n",
    "            plt.show()\n",
    "        if (epoch + 1) % alpha_reduce_each == 0:\n",
    "            alpha *= alpha_reduce_mul\n",
    "\n",
    "model.cpu()\n",
    "synthetic_image = synthetic_image.cpu()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(starting_image)\n",
    "plt.title(\"Starting image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(min_max_scale(synthetic_image[class_ind].detach().numpy().transpose(1, 2, 0)))\n",
    "plt.title(\"Resulting image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7698238",
   "metadata": {
    "cellId": "18pip0gcqtbg9lpmo82rzj"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(30, 15))\n",
    "\n",
    "for ind in range(10):\n",
    "    plt.subplot(2, 5, ind+1)\n",
    "    plt.imshow(min_max_scale(synthetic_image[ind].detach().numpy().transpose(1, 2, 0)))\n",
    "    plt.title(classes[ind])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654968da",
   "metadata": {
    "cellId": "xo9oyqklrco2t3vwi06z9l"
   },
   "outputs": [],
   "source": [
    "mean_tensor = torch.tensor(val_transform.transforms[-1].mean).unsqueeze(0).to(device)\n",
    "std_tensor = torch.tensor(val_transform.transforms[-1].std).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78321b",
   "metadata": {
    "cellId": "cqvrjq1a0va97cizycdr6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "max_generation_epochs = 4000\n",
    "alpha = 4.\n",
    "alpha_reduce_each = 500\n",
    "alpha_reduce_mul = 0.5\n",
    "draw_each = 100\n",
    "chosen_class = \"cat\"\n",
    "reg_coef = 10.\n",
    "\n",
    "class_ind = cl_to_ind[chosen_class]\n",
    "img_shape = val_ds[0][0].shape\n",
    "synthetic_image = torch.nn.Parameter(torch.rand(len(classes), *img_shape), requires_grad=True)\n",
    "starting_image = synthetic_image[class_ind].detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "generated_probs = defaultdict(list)\n",
    "optimized_values = defaultdict(list)\n",
    "xx = np.arange(len(classes))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "synthetic_image = synthetic_image.to(device)\n",
    "synthetic_image.retain_grad()\n",
    "for epoch in trange(max_generation_epochs):\n",
    "    synthetic_image.grad = None\n",
    "    out = model(synthetic_image)\n",
    "    pred_class_logit = out[xx, xx]\n",
    "    regularization = ((synthetic_image.mean((0, 2, 3)) - mean_tensor)**2).sum() + ((synthetic_image.std((0, 2, 3)) - std_tensor)**2).sum()\n",
    "    (pred_class_logit.sum() - reg_coef * regularization).backward()\n",
    "    synthetic_image.data = synthetic_image.data + alpha * synthetic_image.grad\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probs = F.softmax(out, dim=1)\n",
    "        pred_class_prob = probs[xx, xx]\n",
    "        for ind in xx:\n",
    "            generated_probs[ind].append(pred_class_prob[ind].item())\n",
    "            optimized_values[ind].append(pred_class_logit[ind].item())\n",
    "        if (epoch + 1) % draw_each == 0:\n",
    "            clear_output(True)\n",
    "            fig = plt.subplots(2, 1, figsize=(20, 8))\n",
    "            \n",
    "            plt.subplot(211)\n",
    "            for ind in xx:\n",
    "                plt.plot(generated_probs[ind])\n",
    "            plt.title(\"Probability of target class\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.ylabel(\"probability\")\n",
    "            plt.yscale(\"log\")\n",
    "            \n",
    "            plt.subplot(212)\n",
    "            for ind in xx:\n",
    "                plt.plot(optimized_values[ind])\n",
    "            plt.title(\"Logit of target class\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            \n",
    "            plt.show()\n",
    "        if (epoch + 1) % alpha_reduce_each == 0:\n",
    "            alpha *= alpha_reduce_mul\n",
    "\n",
    "model.cpu()\n",
    "synthetic_image = synthetic_image.cpu()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(starting_image)\n",
    "plt.title(\"Starting image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(min_max_scale(synthetic_image[class_ind].detach().numpy().transpose(1, 2, 0)))\n",
    "plt.title(\"Resulting image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4944f",
   "metadata": {
    "cellId": "idv0d8egeyhq206w2ng3a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(30, 15))\n",
    "\n",
    "for ind in range(10):\n",
    "    plt.subplot(2, 5, ind+1)\n",
    "    plt.imshow(min_max_scale(synthetic_image[ind].detach().numpy().transpose(1, 2, 0)))\n",
    "    plt.title(classes[ind])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc893df",
   "metadata": {
    "cellId": "rdunep4gq44s0qz7zhgm7"
   },
   "source": [
    "Чтобы оптимизационными и генеративными методами получить результаты лучше - много всего, поэтому сейчас остановимся здесь. Больше найдёте в специализированных курсах по CV, NLP, генеративкам, etc.\n",
    "\n",
    "Полезные ссылки, если хочется посмотреть/ попробовать ещё:\n",
    "\n",
    "* https://distill.pub/2017/feature-visualization\n",
    "* https://yosinski.com/deepvis\n",
    "* https://github.com/utkuozbulak/pytorch-cnn-visualizations\n",
    "* https://github.com/yosinski/deep-visualization-toolbox\n",
    "* https://github.com/tensorflow/lucid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ae3ea",
   "metadata": {
    "cellId": "u9f3d9urvggrt8x732yts"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "notebookId": "00017dc0-69ed-43c3-b989-8029237a77d0",
  "notebookPath": "Sem5 - Interpretation/sem5_interpretation.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
