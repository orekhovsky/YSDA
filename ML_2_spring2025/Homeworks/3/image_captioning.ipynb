{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Лабораторная работа Image Captioning**\n",
    "*Naumov Anton (Any0019)*\n",
    "\n",
    "*To contact me in telegram: @any0019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "jk8t3bfh88p4gip9j3id",
    "execution_id": "fcc28013-d1ea-4210-b30f-7727ab22f1ca"
   },
   "source": [
    "## Задание\n",
    "В данном задании вашей задачей будет построить простую модель для задачи **Image Captioning** - по изображению (image) сгенерировать текстовый заголовок (caption).\n",
    "Задание проверяется в автоматическом режиме в Я.Контест.\n",
    "\n",
    "В задании будет несколько оцениваемых частей:\n",
    "1. **Подготовка данных для модели** (*4 балла*) --> требуется заполнить пропуски и составить пайплайн предобработки данных\n",
    "2. **Построение модели** (*3 балла*) --> требуется заполнить пропуски и составить пайплайн сборки модели\n",
    "3. **Обучение модели** (*3 балла*) --> требуется заполнить пропуски и составить пайплайн обучения модели\n",
    "4. **Оценка результатов** (*2 балл*) --> требуется заполнить пропуски и получить предсказания модели\n",
    "5. **Валидация качества** (*3+ баллов*) --> вам нужно будет обучить модель, чтобы превзойти определённые пороги на валидационной выборке. Эта часть задания подразумевает возможность получения дополнительных баллов: в остальном ноутбуке вы соберёте модель по некоторой моей рекомендации, а именно в этой части от вас мы ожидаем экспериментов. Попробуйте собрать свою модель, улучшить качество, попробовать другие архитектуры и концептуальные подходы, в общем - удачи! За хорошие попытки полагаются баллы сверх базовой стоимости дз.\n",
    "\n",
    "**Структура данных:** \n",
    "- Вместе с заданием вам предложен файл __dataset.tar.gz__ (смотрите первую ячейку кода в ноутбуке), в нём вы найдёте папку data, в которой присутствуют две папки с изображениями (**train** и **val**) в формате .png и два файла **captions_train.tsv** и **captions_val.tsv**\n",
    "- В файлах captions находятся таблицы с 6 полями, разделёнными через **\\t**, содержащими `img_id` (название файла с изображением в соответствующей папке) и `caption #1-#5` (5 текстовых заголовков для изображения __img_id__)\n",
    "- Не используйте val в обучении модели, только в тестах, т.к. часть баллов в конце будет выдаваться в зависимости от результатов вашей модели на test выборке, а проверка на val выборке - единственный способ оценки и само-проверки перед отправкой в контест на test замер\n",
    "\n",
    "**Концепция простой модели**\n",
    "- Будем рассматривать задачу предсказания следующего слова в предложении, имея изображение и предыдущие слова\n",
    "- Для получения фичей из изображений будем использовать крупную свёрточную предобученную архитектуру\n",
    "- Для получения фичей из текста будем использовать рекуррентную архитектуру с предобученными эмбеддингами, но чтобы передать внутрь этой модельки информацию из изображений будем в качестве hidden_0 передавать в них приведённые линейным слоем к нужному размеру фичи изображений\n",
    "- Классификатор, завершающийся линейным слоем к размеру словаря над финальным набором фичей\n",
    "- LogSoftmax + NLLLoss (или CrossEntropyLoss) для оценки предсказания\n",
    "\n",
    "**Модели большие, подсчёты не быстрые - закладывайте время на обучение моделей**\n",
    "\n",
    "**В ноутбуке будет какое-то количество assert-ов, их задача - подсказать вам, но совсем не всегда вы должны строго их проходить**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "9jvseik14wk71g2wwaq3ug",
    "execution_id": "78fad2bc-2e30-4424-9085-e4863bda95c9"
   },
   "source": [
    "------------------------------\n",
    "\n",
    "***Полезный комментарий:***\n",
    "\n",
    "*Местами, в коде вы будете встречать выполнение bash скриптов - их легко распознать по комментарию `#!:bash` вверху ячейки.*\n",
    "\n",
    "*Если вы пользуетесь датосферой, то оставьте как есть, всё сработает, а вот если Google colab или что-то ещё, то замените код скриптов так, чтобы каждая его строка начиналась с `!`.*\n",
    "\n",
    "*К примеру:*\n",
    "\n",
    "*Изначальный блок:*\n",
    "```bash\n",
    "#!:bash\n",
    "ls -sh dataset.tar\n",
    "file dataset.tar\n",
    "```\n",
    "\n",
    "*Заменённый блок:*\n",
    "```bash\n",
    "!ls -sh dataset.tar\n",
    "!file dataset.tar\n",
    "```\n",
    "\n",
    "...вроде как сейчас второй вариант так же работает и в датасфере, но я не уверен"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "s89gqf90esoy0y0s6lgusf",
    "execution_id": "ea7b88d5-ae9e-4048-817a-260c4599c717"
   },
   "source": [
    "## 0. Скачиваем и распаковываем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "72s9d8vmx14mhf5ip5cid",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
    "public_key = 'https://disk.yandex.ru/d/W9EODR61Dj1Oxg'\n",
    "\n",
    "# Получаем загрузочную ссылку\n",
    "final_url = base_url + urlencode(dict(public_key=public_key))\n",
    "response = requests.get(final_url)\n",
    "download_url = response.json()['href']\n",
    "\n",
    "# Загружаем файл и сохраняем его\n",
    "download_response = requests.get(download_url)\n",
    "with open('dataset.tar.gz', 'wb') as f:   # Здесь укажите нужный путь к файлу\n",
    "    f.write(download_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "6lc8kepcrfaawdaiicq8z",
    "execution_id": "478f1651-5962-4b1f-8dae-9f6025256433",
    "tags": []
   },
   "source": [
    "## 1. Подготовка данных (***4 балла***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "e0m5dp771bdh33aaleybr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!:bash\n",
    "ls -sh dataset.tar.gz\n",
    "file dataset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "44s07i91esdf6su3dzydy8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!:bash\n",
    "tar xfz dataset.tar.gz -C ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "458ruqajt4bbd05hzaxj9m",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "vb3smxfxe8i1onq667fx0f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = \"./data\"  # укажите здесь путь до распакованных данных\n",
    "\n",
    "dfs = dict()\n",
    "for split in ['train', 'val']:\n",
    "    dfs[split] = pd.read_csv(os.path.join(data_folder, f'captions_{split}.tsv'), sep='\\t')\n",
    "\n",
    "dfs['train'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "v5qwinuw6d1vnk6r2hx6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "9kgn568oinho52gko0aik",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs['val'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "nxxi93ojkqspdw063jxfi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "# Для чтения изображений из файлов мы будем использовать библиотеку cv2 --> всё что вам нужно знать\n",
    "#   функция cv2.imread(path) принимает на вход путь к файлу изображения и возвращает np.array с изображением\n",
    "#   в порядке h x w x c\n",
    "image = cv2.imread(os.path.join(data_folder, 'train', '0001.png'))\n",
    "print(type(image))\n",
    "print(image.shape)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "e6uhvmrb4rvzmh7iifb6u",
    "tags": []
   },
   "outputs": [],
   "source": [
    "inds = list(range(10))\n",
    "split = 'train'\n",
    "\n",
    "h, w = 2, 5\n",
    "title_width = 43\n",
    "\n",
    "assert h*w >= len(inds)\n",
    "\n",
    "fig, ax = plt.subplots(h, w, figsize=(20, 15))\n",
    "\n",
    "for i, ind in enumerate(inds):\n",
    "    row = dfs[split].iloc[ind]\n",
    "    img_id = row['img_id']\n",
    "    captions = [row[f'caption #{i}'] for i in range(5)]\n",
    "    \n",
    "    caption_adjasted = map(lambda el: '\\n'.join([(str(el[0]) + ': ' + el[1])[k:k+title_width] for k in range(0, 3 + len(el[1]), title_width)]), enumerate(captions))\n",
    "    caption = '\\n'.join(caption_adjasted)\n",
    "    plt.subplot(h, w, i+1)\n",
    "    plt.title(caption)\n",
    "    plt.imshow(cv2.imread(os.path.join(data_folder, split, img_id)))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "yaujznd4gtr816600r8rmk"
   },
   "source": [
    "### 1.2 Предобработка изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "573iruu84bg8f33goxk6n"
   },
   "source": [
    "**Сперва напишем предобработку для изображений**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "b72e3qg96vvvc1z36uxlub",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ajgf8dcqmkiur5fcn1tynd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Подготовьте функцию для аугументации одного изображения\n",
    "\n",
    "# !!! Напомню, что в выборе аугументаций многое будет зависеть от того,\n",
    "# !!!  на каких аугументациях училась предобученная модель, которую вы возьмёте,\n",
    "# !!!  так что рекомендую принимать решение о финальных аугументациях после изучения\n",
    "# !!!  выбранной вами модели\n",
    "\n",
    "# Поканальное среднее и стандартное отклонение (как и откуда лучше взять?)\n",
    "channel_mean = np.array(...)\n",
    "channel_std = np.array(...)\n",
    "\n",
    "image_prepare = tr.Compose([\n",
    "    tr.ToPILImage(),\n",
    "    # Любые преобразования, которые вы захотите:\n",
    "    #   https://pytorch.org/vision/stable/transforms.html\n",
    "    ...,\n",
    "    tr.ToTensor(),\n",
    "    ...,\n",
    "    tr.Normalize(mean=channel_mean, std=channel_std),\n",
    "])\n",
    "\n",
    "# Для валидации рекомендую использовать минимальное количество аугументаций, чтобы\n",
    "#  замеряться честно - изменения размера, нормализация (все случайные аугументации\n",
    "#  не делайте для валидации и обязательно для обучения делайте со средним в нуле)\n",
    "image_prepare_val = tr.Compose([\n",
    "    tr.ToPILImage(),\n",
    "    ...,\n",
    "    tr.ToTensor(),\n",
    "    ...,\n",
    "    tr.Normalize(mean=channel_mean, std=channel_std),\n",
    "])\n",
    "\n",
    "\n",
    "# Визуализация ваших преобразований на одном изображении (проверка на адекватность)\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "image = cv2.imread(os.path.join(data_folder, \"train\", \"0001.png\"))\n",
    "\n",
    "def de_normalize(img):\n",
    "    return minmax_scale(\n",
    "        (img.reshape(3, -1) + channel_mean[:, None]) * channel_std[:, None],\n",
    "        feature_range=(0., 1.),\n",
    "        axis=1,\n",
    "    ).reshape(*img.shape)\n",
    "\n",
    "# Проверка на адекватность\n",
    "fig, ax = plt.subplots(2, 4, figsize=(26, 12))\n",
    "for i in range(8):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    transformed_image = image_prepare(image).numpy().transpose(1, 2, 0)\n",
    "    to_show_image = de_normalize(transformed_image)\n",
    "    plt.imshow(to_show_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ntgyvywgkhq8n4kw8m2uo3"
   },
   "source": [
    "### 1.3 Предобработка заголовков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "83wlsl4e6riqlup3gh4yf"
   },
   "source": [
    "**Затем напишем предобработку для заголовков**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xpog1107mvivegx52yvw2d",
    "execution_id": "d5a48903-1cec-4a5c-8a9b-1873f654a37b"
   },
   "source": [
    "Для простоты вычислений предлагаю сделать крайне простую токенизацию, пользуясь регулярными выражениями и библиотекой **re**\n",
    "\n",
    "1. Приводим текст к нижнему регистру\n",
    "2. Заменяем всю пунктуацию на пробелы\n",
    "3. Убираем пробельные символы с концов строки\n",
    "4. Разбиваем по ненулевой последовательности пробельных символов\n",
    "5. Добавляем специальные токены \\<BOS> (begining of sentence) и \\<EOS> (end of sentence), чтобы обозначить границы заголовка для модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "h9hk3bpjfy84jg6s1etmgn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    ...\n",
    "\n",
    "# Проверка вашей функции\n",
    "tokenize(\"My name is Any0019, I wish you good luck in this lab! Bye-bye :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "uir8tvlv9qq29ei0avg3xh",
    "execution_id": "5e85a9d2-7ebc-4a3e-9c10-ce72a2d9ba8b"
   },
   "source": [
    "Соберём словарь из всех слов, что встречаются в заголовках в train\n",
    "1. Токенизируем заголовки\n",
    "2. Обновляем частоту всех отдельных токенов\n",
    "3. Выкидываем все слова встретившиеся меньше `MIN_FREQ` (допустим 3 - можете взять другое число) -> заменяем на специальный `<UNK>` (unknown) токен\n",
    "4. Записываем быстрое преобразование из токенов в индексы и наоборот"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "lgvmpv8nq478sjrkw7kpo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import trange\n",
    "from collections import Counter\n",
    "\n",
    "# Посчитаем частоту встречаемости различных токенов (не нужно для bos и eos)\n",
    "vocab_freq = Counter()\n",
    "\n",
    "# Параллельно заодно посчитаем длины заголовков в токенах (сколько раз встречалась какая длина в токенах)\n",
    "#  sizes[5] = число заголовков из 5 токенов (без учёта bos и eos)\n",
    "sizes = Counter()\n",
    "for i in trange(len(dfs['train'])):\n",
    "    ...\n",
    "\n",
    "global_max_seq_len = np.max(list(sizes.keys()))\n",
    "\n",
    "show_ = 20\n",
    "fig, ax = plt.subplots(3, 1, figsize=(20, 12))\n",
    "\n",
    "plt.subplot(312)\n",
    "vocab_freq = {k: v for k, v in sorted(vocab_freq.items(), key=lambda item: item[1])}\n",
    "plt.title('least popular words')\n",
    "plt.bar(list(vocab_freq.keys())[:show_], list(vocab_freq.values())[:show_])\n",
    "\n",
    "plt.subplot(311)\n",
    "vocab_freq = {k: v for k, v in sorted(vocab_freq.items(), key=lambda item: item[1], reverse=True)}\n",
    "plt.title('most popular words')\n",
    "plt.bar(list(vocab_freq.keys())[:show_], list(vocab_freq.values())[:show_])\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.title('sequence sizes')\n",
    "plt.bar(list(sizes.keys()), list(sizes.values()))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "kepravbotpxrj4skman8g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MIN_FREQ = ...  # токены с частотой ниже этой константы заменяются на <UNK>\n",
    "\n",
    "# Так же добавляем <PAD> токен для паддингов\n",
    "tok_to_ind = {\n",
    "    '<UNK>': 0,\n",
    "    '<BOS>': 1,\n",
    "    '<EOS>': 2,\n",
    "    '<PAD>': 3,\n",
    "}\n",
    "\n",
    "ind_to_tok = {\n",
    "    0: '<UNK>',\n",
    "    1: '<BOS>',\n",
    "    2: '<EOS>',\n",
    "    3: '<PAD>',\n",
    "}\n",
    "\n",
    "# Заполнить оставшееся\n",
    "...\n",
    "\n",
    "assert len(tok_to_ind) == len(ind_to_tok)\n",
    "vocab_size = len(tok_to_ind)\n",
    "print(f\"Resulting vocab size: {vocab_size} (out of {len(vocab_freq)} tokens overall, due to MIN_FREQ={MIN_FREQ})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "1hhl4pik5fp4040p20u08s",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Функция возвращает по тексту индексы токенов в тексте\n",
    "def to_ids(text):\n",
    "    ...\n",
    "\n",
    "text = \"I really hope that you can get a lot of great results with this lab! You're awesome! I believe in you, good luck :)\"\n",
    "toks = tokenize(text)\n",
    "ids = to_ids(text)\n",
    "\n",
    "print(list(zip(toks, ids)))\n",
    "\n",
    "assert toks[0] == '<BOS>' and toks[-1] == '<EOS>'\n",
    "assert ids[0] == tok_to_ind['<BOS>'] and ids[-1] == tok_to_ind['<EOS>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xxcu5ctdfz233tyafpj1u"
   },
   "source": [
    "### 1.4 Датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "upzb9xjar21r1cqy8zjnt"
   },
   "source": [
    "**Сделаем класс датасета в стиле torch.utils.data.Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "amlltts14prhvc45zm61i9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "    \"\"\"\n",
    "        imgs_path ~ путь к папке с изображениями\n",
    "        captions_path ~ путь к .tsv файлу с заголовками изображений\n",
    "    \"\"\"\n",
    "    def __init__(self, imgs_path, captions_path, train=True):\n",
    "        super(ImageCaptioningDataset).__init__()\n",
    "        # Читаем и записываем из файлов в память класса, чтобы быстро обращаться внутри датасета\n",
    "        # Если не хватает памяти на хранение всех изображений, то подгружайте прямо во время __getitem__, но это замедлит обучение\n",
    "        # Проведите всю предобработку, которую можно провести без потери вариативности датасета, здесь\n",
    "        ...\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ...\n",
    "        \n",
    "        # Получаем предобработанное изображение (не забудьте отличие при train=True или train=False)\n",
    "        ...\n",
    "        \n",
    "        # Берём все заголовки или только один случайный (случайность должна происходить при каждом вызове __getitem__, \n",
    "        #  чтобы во время обучения вы в разных эпохах могли видеть разные заголовки для одного изображения)\n",
    "        ...\n",
    "        \n",
    "        return img, captions\n",
    "    \n",
    "    def __len__(self):\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "hr1yjxjlh462dzuavnjj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_train = ImageCaptioningDataset(\n",
    "    os.path.join(data_folder, 'train'),\n",
    "    os.path.join(data_folder, f'captions_train.tsv'),\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "ds_val = ImageCaptioningDataset(\n",
    "    os.path.join(data_folder, 'val'),\n",
    "    os.path.join(data_folder, f'captions_val.tsv'),\n",
    "    train=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "skxtz0k40v01d463pe3jou",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img1, captions1 = ds_train[0]\n",
    "img2, captions2 = ds_train[1]\n",
    "\n",
    "# Изображения должны быть тензорами (желательно одного размера для удобства складывания в батч)\n",
    "assert isinstance(img1, torch.Tensor)\n",
    "assert isinstance(img2, torch.Tensor)\n",
    "assert img1.shape == img2.shape  # может быть не так, только если вы понимаете что делаете\n",
    "assert img1.shape[0] == 3\n",
    "print(f\"Размер изображения из датасета: {img1.shape}\")\n",
    "\n",
    "# На этом этапе предлагаю возвращать просто list-ы captions без паддинга\n",
    "assert isinstance(captions1, list)\n",
    "assert isinstance(captions2, list)\n",
    "\n",
    "# Если вы возвращаете все или >1 заголовков для каждого изображения\n",
    "assert isinstance(captions1[0], list) and len(captions1) == len(captions2)\n",
    "assert isinstance(captions1[0][0], int)\n",
    "assert ind_to_tok[captions1[0][0]] == \"<BOS>\"\n",
    "assert ind_to_tok[captions1[0][-1]] == \"<EOS>\"\n",
    "print(f\"Число заголовков для одного изображения: {len(captions1)}\")\n",
    "\n",
    "# Если вы возвращаете 1 случайный заголовок для каждого изоброажения\n",
    "# assert isinstance(captions1[0], int)\n",
    "# assert ind_to_tok[captions1[0]] == \"<BOS>\"\n",
    "# assert ind_to_tok[captions1[-1]] == \"<EOS>\"\n",
    "\n",
    "# Проверка что число элементов в датасете совпадает с соответствующим числом изображений\n",
    "assert len(ds_train) == len(os.listdir(os.path.join(data_folder, 'train')))\n",
    "assert len(ds_val) == len(os.listdir(os.path.join(data_folder, 'val')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "8y7ewpjb8v920hrn0e9p19",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Проверка на адекватность\n",
    "plt.imshow(\n",
    "    de_normalize(img1.numpy().transpose(1, 2, 0))\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "a58dayy93gjik14bml66v9"
   },
   "source": [
    "### 1.5 Даталоадер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "xgjy1gdmzf8imgku7ree7a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Здесь хотим задать кастомную функцию для того, как именно складывать данные в батч\n",
    "# Эта функция позже будет передана в collate_fn аргумент даталоадера и будет отвечать за то,\n",
    "#  как обработать батч и превратить его в тензоры нужного вида\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Функция получает на вход batch - представляет из себя List[el], где каждый el - один вызов __getitem__\n",
    "    #  вашего датасета\n",
    "    # На выход вы выдаёте то, что будет выдавать Dataloader на каждом next() из генератора - вы хотите иметь на выходе\n",
    "    #  несколько тензоров\n",
    "    \n",
    "    # Моё предложение по тому как должен выглядеть батч на выходе:\n",
    "    #   img_batch: [batch_size, num_channels, height, width] --> сложенные в батч изображения\n",
    "    #   captions_batch: [batch_size, num_captions_per_image, max_seq_len or local_max_seq_len] --> сложенные в\n",
    "    #       батч заголовки при помощи padding-а\n",
    "    ...\n",
    "    return img_batch, captions_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ibp7abd576q0xx46k10zz0k",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 0\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset=ds_train,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    dataset=ds_val,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "cu7pbmxv37hipll7777wf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_batch, captions_batch = next(iter(dataloader_train))\n",
    "\n",
    "assert isinstance(img_batch, torch.Tensor)\n",
    "assert isinstance(captions_batch, torch.Tensor)\n",
    "\n",
    "assert img_batch.shape[:2] == torch.Size([batch_size, 3])\n",
    "assert captions_batch.shape[0] == batch_size\n",
    "assert len(captions_batch.shape) in [2, 3]\n",
    "\n",
    "assert isinstance(captions_batch.reshape(-1)[0].item(), int)\n",
    "\n",
    "print(\"Размер батча изображений: {}\\nРазмер батча заголовков: {}\".format(img_batch.shape, captions_batch.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "cw2vcmul7261h2ipvuhhf4",
    "execution_id": "9dce1b0e-14af-409f-845a-7904b688a675"
   },
   "source": [
    "## 2. Составляем модель (***3 балла***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Картинка предложенной архитектуры модели для понимания](https://disk.yandex.ru/i/sHzk7LBP8-A5aQ)\n",
    "\n",
    "<img src=\"https://disk.yandex.ru/i/sHzk7LBP8-A5aQ\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "y92usx83bnb5zp3qip6a15"
   },
   "source": [
    "### 2.1 Фича-экстрактор для изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "w8ymxl1w1cf1q4nugxn4n"
   },
   "source": [
    "Возьмите какую-нибудь предобученную модель (к примеру resnet), по желанию заморозьте все или часть слоёв, наиболее вероятно уберите последний слой\n",
    "\n",
    "```python\n",
    "# выбрать веса нужной вам модели\n",
    "weights = models.ResNet18_Weights.DEFAULT\n",
    "\n",
    "# скачать нужную архитекутру модели и инициализировать её весами\n",
    "model = models.resnet18(weights=weights, progress=True)\n",
    "\n",
    "# посмотреть какие аугументации использовались в данной модели\n",
    "weights.transforms()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "x22s6ju2nyaf7wzq301eva",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "qh3qk4xr9q130okh60u59a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class img_fe_class(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super(img_fe_class, self).__init__()\n",
    "        ...\n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ho5hu57qch9rwbezi07yz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_fe = img_fe_class(...)\n",
    "    \n",
    "img_features = img_fe(img_batch)\n",
    "\n",
    "assert len(img_features.shape) == 2\n",
    "assert img_features.shape[0] == img_batch.shape[0]\n",
    "\n",
    "print(f'Для изображения возвращает {img_features.shape[1]} фичей')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3okjqfglghe65rxjkhcj5t"
   },
   "source": [
    "### 2.2 Фича-экстрактор для текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "twueftwg4b6oodu4roj6c",
    "execution_id": "9a0c4261-6372-4aec-b25d-e3627bad4720"
   },
   "source": [
    "Давайте скачаем предобученные glove вектора и инициализируем nn.Embedding ими, там где мы их знаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "28xxgrbk20boyf98zvj7i9"
   },
   "outputs": [],
   "source": [
    "#!:bash\n",
    "wget -O glove.zip https://huggingface.co/stanfordnlp/glove/resolve/main/glove.840B.300d.zip\n",
    "# mirror https://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip\n",
    "\n",
    "unzip glove.zip\n",
    "\n",
    "ls -sh glove.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "bzkg45inz9c8zhrit7f9y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Открываем glove\n",
    "np.random.seed(19)\n",
    "\n",
    "def load_glove_weights(file_path, vocab, pad_token=\"<PAD>\"):\n",
    "    print(\"Loading Glove Weights\")\n",
    "    # Инициализируем веса для всех слов стандартным нормальным распределением\n",
    "    glove_weights = np.random.uniform(0, 1, (len(vocab), 300))\n",
    "    mask_found = np.zeros(len(vocab), dtype=bool)\n",
    "    \n",
    "    # Ищем среди слов в предобученном glove слова из нашего словаря\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in tqdm(f, total=2196018):\n",
    "            line = line.split()\n",
    "            token = ' '.join(line[:-300])\n",
    "            embed = line[-300:]\n",
    "\n",
    "            # Если нашли, то подменяем эмбеддинг из glove\n",
    "            if token in vocab:\n",
    "                ind = vocab[token]\n",
    "                mask_found[ind] = True\n",
    "                glove_weights[ind, :] = np.array(list(map(float, embed)), dtype=np.float)\n",
    "\n",
    "    print(f\"{mask_found.sum()} words from vocab of size {len(vocab)} loaded!\")\n",
    "\n",
    "    glove_weights[vocab[pad_token]] = np.zeros(300, dtype=np.float)\n",
    "    return glove_weights, mask_found\n",
    "\n",
    "\n",
    "glove_path = \"/home/jupyter/datasphere/project/Sem2 - NLP/glove.840B.300d.txt\"\n",
    "glove_weights, mask_found = load_glove_weights(glove_path, tok_to_ind, \"<PAD>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Теперь, давайте составим рекурентную модельку для получения фичей из текстов с учётом картинок\n",
    "\n",
    "Используйте эмбеддинги из картинок в качестве начального состояния скрытого слоя рекурентной нейросети (h_0), если размерности не совпадают, то линейный слой вам в помощь. Если в вашей rnn-like ячейке несколько слоёв, то подавайте в каждый из них.\n",
    "\n",
    "Вам может пригодиться:\n",
    "\n",
    "1. библиотека einops - один из самых удобных способов перетасовывать размерности в тензорах, пример:\n",
    "```python\n",
    "from einops import rearrange\n",
    "\n",
    "a = torch.zeros(batch_size, num_captions, max_seq_length, emb_size)\n",
    "b = rearrange(a, \"bs cap seq emb -> (bs cap) seq emb\")  # (batch_size * num_captions, max_seq_length, emb_size)\n",
    "c = rearrange(b, \"(bs cap) seq emb -> bs cap seq emb\", cap=num_captions)  # (batch_size, num_captions, max_seq_length, emb_size)\n",
    "```\n",
    "\n",
    "2. torch.Tensor.repeat() - размножить тензор вдоль одной из размерностей, пример:\n",
    "```python\n",
    "a = torch.zeros(3, 5)\n",
    "b = a[None, :, None, :]  # (1, 3, 1, 5)\n",
    "c = b.repeat(2, 1, 4, 2)  # (2, 3, 4, 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "iieb90aa3yqh9h8f7zaii",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "class text_fe_class(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super(text_fe_class, self).__init__()\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=300, padding_idx=tok_to_ind['<PAD>'])\n",
    "        self.embed.weight = nn.Parameter(\n",
    "            torch.from_numpy(glove_weights).to(dtype=self.embed.weight.dtype),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "        ...\n",
    "        \n",
    "    def forward(self, texts, img_features):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "c63jp4cbtoorr1jyym2wnm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_fe = text_fe_class(...)\n",
    "text_features = text_fe(captions_batch, img_features)\n",
    "\n",
    "assert text_features.shape[:-1] == captions_batch.shape\n",
    "assert len(text_features.shape) in [3, 4]\n",
    "assert len(text_features.shape) == len(captions_batch.shape) + 1\n",
    "\n",
    "print(captions_batch.shape)\n",
    "print(img_features.shape)\n",
    "print(text_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "flnbv9uwq6qgzshqfrnus"
   },
   "source": [
    "### 2.3 Финальная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "mk0g1k22smb9ubu2pno0pj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class image_captioning_model(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super(image_captioning_model, self).__init__()\n",
    "        ...\n",
    "        \n",
    "    def forward(self, img_batch, texts_batch):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "06hl0s2t9k4xoiqb6ry03i",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = image_captioning_model(...)\n",
    "\n",
    "res = model(img_batch, captions_batch)\n",
    "\n",
    "assert res.shape[:-1] == captions_batch.shape\n",
    "assert res.shape[-1] == vocab_size\n",
    "\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "jnpoagcrohh9jaz2jco0b",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "from collections import defaultdict\n",
    "\n",
    "def beautiful_int(i):\n",
    "    i = str(i)\n",
    "    return \".\".join(reversed([i[max(j, 0):j+3] for j in range(len(i) - 3, -3, -3)]))\n",
    "\n",
    "# Считаем общее число параметров в нашей модели\n",
    "def model_num_params(model, verbose_all=True, verbose_only_learnable=False):\n",
    "    sum_params = 0\n",
    "    sum_learnable_params = 0\n",
    "    submodules = defaultdict(lambda : [0, 0])\n",
    "    for name, param in model.named_parameters():\n",
    "        num_params = np.prod(param.shape)\n",
    "        if verbose_all or (verbose_only_learnable and param[1].requires_grad):\n",
    "            print(\n",
    "                colored(\n",
    "                    '{: <42} ~  {: <9} params ~ grad: {}'.format(\n",
    "                        name,\n",
    "                        beautiful_int(num_params),\n",
    "                        param.requires_grad,\n",
    "                    ),\n",
    "                    {True: \"green\", False: \"red\"}[param[1].requires_grad],\n",
    "                )\n",
    "            )\n",
    "        sum_params += num_params\n",
    "        sm = name.split(\".\")[0]\n",
    "        submodules[sm][0] += num_params\n",
    "        if param.requires_grad:\n",
    "            sum_learnable_params += num_params\n",
    "            submodules[sm][1] += num_params\n",
    "    print(\n",
    "        f'\\nIn total:\\n  - {beautiful_int(sum_params)} params\\n  - {beautiful_int(sum_learnable_params)} learnable params'\n",
    "    )\n",
    "    \n",
    "    for sm, v in submodules.items():\n",
    "        print(\n",
    "            f\"\\n . {sm}:\\n .   - {beautiful_int(submodules[sm][0])} params\\n .   - {beautiful_int(submodules[sm][1])} learnable params\"\n",
    "        )\n",
    "    return sum_params, sum_learnable_params\n",
    "\n",
    "\n",
    "sum_params, sum_learnable_params = model_num_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "x10a7wm1m4j9ep99gz8y3a",
    "execution_id": "a60a3d7a-5015-42a6-b1a1-28ea81ba5647"
   },
   "source": [
    "## 3. Пайплайн обучения (***3 балла***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "rfmzra7hz28zqm3cj2vv6"
   },
   "source": [
    "### 3.1 Оптимайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "1p9n5u1z2a90943yahtgkpn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def create_model_and_optimizer(model_class, model_params, ..., device=device):\n",
    "    model = model_class(**model_params)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = ...\n",
    "    return model, optimizer\n",
    "\n",
    "# Убедитесь что всё сработало и создалось нормально и без ошибок\n",
    "model, optimizer = create_model_and_optimizer(\n",
    "    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "8ss4qmu9tiuz305c4ppe7t"
   },
   "source": [
    "### 3.2 Один шаг обучения/валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помните что captions вы используете и как input (`captions[..., seq->0:-1, ...]`) в модель и как target (`captions[..., seq->1:None, ...]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "zwblqwgw1g8c97zamvy4y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def train(model, opt, loader, criterion):\n",
    "    model.train()\n",
    "    losses_tr = []\n",
    "    for img_batch, captions_batch in tqdm(loader):\n",
    "        img_batch = img_batch.to(device)\n",
    "        captions_batch = captions_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = ...\n",
    "        target = ...\n",
    "        loss = ...\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses_tr.append(loss.item())\n",
    "    \n",
    "    return model, optimizer, np.mean(losses_tr)\n",
    "\n",
    "\n",
    "def val(model, loader, criterion, metric_names=None):\n",
    "    model.eval()\n",
    "    losses_val = []\n",
    "    if metric_names is not None:\n",
    "        metrics = defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "        for img_batch, captions_batch in tqdm(loader):\n",
    "            img_batch = img_batch.to(device)\n",
    "            captions_batch = captions_batch.to(device)\n",
    "            \n",
    "            pred = ...\n",
    "            target = ...\n",
    "            loss = ...\n",
    "\n",
    "            losses_val.append(loss.item())\n",
    "            \n",
    "            # Можете добавить сюда любые метрики, которые хочется (см. код здесь и 3.3 за подробностями)\n",
    "            if metric_names is not None:\n",
    "                if 'accuracy' in metric_names:\n",
    "                    preds = torch.argsort(pred, dim=-1, descending=True)\n",
    "                    for k in metric_names[\"accuracy\"][\"top\"]:\n",
    "                        metrics[f'accuracy ~ top#{k}'].append(\n",
    "                            np.mean([target[i] in preds[i, :k] for i in range(target.shape[0])])\n",
    "                        )\n",
    "\n",
    "        if metric_names is not None:\n",
    "            for name in metrics:\n",
    "                metrics[name] = np.mean(metrics[name])\n",
    "    \n",
    "    return np.mean(losses_val), metrics if metric_names else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "hqv3jmnfx7o2ep9obsjt5"
   },
   "source": [
    "### 3.3 Цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "jn68d89hk6yv3ghn46vg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def learning_loop(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    scheduler=None,\n",
    "    min_lr=None,\n",
    "    epochs=10,\n",
    "    val_every=1,\n",
    "    draw_every=1,\n",
    "    separate_show=False,\n",
    "    model_name=None,\n",
    "    chkp_folder=\"./chkps\",\n",
    "    metric_names=None,\n",
    "):\n",
    "    # Выбираем куда будем сохранять модель\n",
    "    if model_name is None:\n",
    "        if os.path.exists(chkp_folder):\n",
    "            num_starts = len(os.listdir(chkp_folder)) + 1\n",
    "        else:\n",
    "            num_starts = 1\n",
    "        model_name = f'model#{num_starts}'\n",
    "    else:\n",
    "        if \"#\" not in model_name:\n",
    "            model_name += \"#0\"\n",
    "    changed = False\n",
    "    while os.path.exists(os.path.join(chkp_folder, model_name + '.pt')):\n",
    "        model_name, ind = model_name.split(\"#\")\n",
    "        model_name += f\"#{int(ind) + 1}\"\n",
    "        changed = True\n",
    "    if changed:\n",
    "        warnings.warn(f\"Selected model_name was used already! To avoid possible overwrite - model_name changed to {model_name}\")\n",
    "        \n",
    "    # Инициализируем переменные\n",
    "    losses = {'train': [], 'val': []}\n",
    "    lrs = []\n",
    "    best_val_loss = np.Inf\n",
    "    if metric_names is not None:\n",
    "        metrics = defaultdict(list)\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    # Цикл обучения\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f'#{epoch}/{epochs}:')\n",
    "\n",
    "        lrs.append(get_lr(optimizer))\n",
    "        \n",
    "        model, optimizer, loss = train(model, optimizer, train_loader, criterion)\n",
    "        losses['train'].append(loss)\n",
    "\n",
    "        # Каждые val_every эпох проводим валидацию\n",
    "        if not (epoch % val_every):\n",
    "            loss, metrics_ = val(model, val_loader, criterion, metric_names=metric_names)\n",
    "            losses['val'].append(loss)\n",
    "            if metrics_ is not None:\n",
    "                for name, value in metrics_.items():\n",
    "                    metrics[name].append(value)\n",
    "            \n",
    "            # Сохраняем лучшую по валидации модель\n",
    "            if loss < best_val_loss:\n",
    "                if not os.path.exists(chkp_folder):\n",
    "                    os.makedirs(chkp_folder)\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict(),\n",
    "                        'losses': losses,\n",
    "                    },\n",
    "                    os.path.join(chkp_folder, model_name + '.pt'),\n",
    "                )\n",
    "                best_val_loss = loss\n",
    "            \n",
    "            # Шаг шедулера\n",
    "            if scheduler:\n",
    "                try:\n",
    "                    scheduler.step()\n",
    "                except:\n",
    "                    scheduler.step(loss)\n",
    "\n",
    "        # Каждые draw_every эпох рисуем графики\n",
    "        if not (epoch % draw_every):\n",
    "            clear_output(True)\n",
    "            ww = 3 if separate_show else 2\n",
    "            ww_metrics = 0\n",
    "            if metric_names is not None:\n",
    "                plot_ids_ = [\n",
    "                    [key, metric_meta.get(\"plot_id\", 1), metric_meta]\n",
    "                    for key, metric_meta\n",
    "                    in metric_names.items()\n",
    "                ]\n",
    "                ww_metrics = len(set(el[1] for el in plot_ids_))\n",
    "                assert all(isinstance(el[1], int) for el in plot_ids_)\n",
    "                assert all(el[1] <= ww_metrics for el in plot_ids_)\n",
    "                assert all(el[1] >= 1 for el in plot_ids_)\n",
    "                \n",
    "                plot_ids = defaultdict(list)\n",
    "                for el in plot_ids_:\n",
    "                    plot_ids[el[1]].append((el[0], el[2]))\n",
    "                \n",
    "            fig, ax = plt.subplots(1, ww + ww_metrics, figsize=(30, 10))\n",
    "            fig.suptitle(f'#{epoch}/{epochs} ~ {timedelta(seconds=time.monotonic() - start_time)}')\n",
    "\n",
    "            plt.subplot(1, ww + ww_metrics, 1)\n",
    "            plt.plot(losses['train'], 'r.-', label='train')\n",
    "            if separate_show:\n",
    "                plt.title('loss on train')\n",
    "                plt.legend()\n",
    "            plt.grid()\n",
    "\n",
    "            if separate_show:\n",
    "                plt.subplot(1, ww + ww_metrics, 2)\n",
    "                plt.title('loss on validation')\n",
    "                plt.grid()\n",
    "            else:\n",
    "                plt.title('losses')\n",
    "            plt.plot(losses['val'], 'g.-', label='val')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, ww + ww_metrics, ww)\n",
    "            plt.title('learning rate')\n",
    "            plt.plot(lrs, 'g.-', label='lr')\n",
    "            plt.yscale(\"log\")\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            \n",
    "            if metric_names is not None:\n",
    "                for plot_id, keys_meta in plot_ids.items():\n",
    "                    aggregated_meta = {}\n",
    "                    plt.subplot(1, ww + ww_metrics, ww + plot_id)\n",
    "                    if len(keys_meta) > 1:\n",
    "                        plt.title(f'additional metrics #{plot_id}')\n",
    "                    elif len(keys_meta) == 1:\n",
    "                        plt.title(keys_meta[0][0])\n",
    "                    for key_meta in keys_meta:\n",
    "                        key, meta = key_meta\n",
    "                        for meta_key in [\"yscale\"]:\n",
    "                            if meta_key in meta:\n",
    "                                assert meta_key not in aggregated_meta, f\"Bad meta data '{meta_key}' doubled inside one plot_id ({plot_id})\"\n",
    "                                aggregated_meta[meta_key] = meta[meta_key]\n",
    "                        for name in metrics:\n",
    "                            if key in name:\n",
    "                                plt.plot(metrics[name], '.-', label=name)\n",
    "                    plt.yscale(aggregated_meta.get(\"yscale\", \"linear\"))\n",
    "                    plt.legend()\n",
    "                    plt.grid()\n",
    "            plt.show()\n",
    "        \n",
    "        # early_stopping - останавливаем обучение, если LR упал ниже min_lr\n",
    "        if min_lr and get_lr(optimizer) <= min_lr:\n",
    "            print(f'Learning process ended with early stop after epoch {epoch}')\n",
    "            break\n",
    "    \n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "dd1po7rfl5m3d6o6xl34yt",
    "execution_id": "374b479c-7b61-4694-9894-ed892a610205"
   },
   "source": [
    "### 3.4 Обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "9m6p1o20os3rvuman6s7c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model, optimizer = create_model_and_optimizer(...)\n",
    "\n",
    "scheduler = ...\n",
    "\n",
    "criterion = ...\n",
    "\n",
    "model, optimizer, losses = learning_loop(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    train_loader = dataloader_train,\n",
    "    val_loader = dataloader_val,\n",
    "    criterion = criterion,\n",
    "    scheduler = scheduler,\n",
    "    epochs = ...,\n",
    "    min_lr = ...,\n",
    "    val_every = 1,\n",
    "    draw_every = 1,\n",
    "    separate_show = False,\n",
    "    metric_names = {\n",
    "        \"accuracy\": {\"top\": [1, 5], \"plot_id\": 1},\n",
    "    },\n",
    "    chkp_folder = \"./chkp\",\n",
    "    model_name = \"default\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ppwwtmipqwgk3smnigxh8"
   },
   "source": [
    "### 3.5 Загрузка чекпоинта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "24do65l9uc3c1en9puqp"
   },
   "source": [
    "#### **Не запускайте этот блок, если не понимаете для чего это в данную секунду!**\n",
    "\n",
    "#### **Так можно случайно перезатереть несколько часов вычислений, если не скопировать их в отдельную переменную/чекпоинт**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "b3cofy2t6del4sktn51mwm"
   },
   "outputs": [],
   "source": [
    "assert False, \"Are you sure? If not - stop right here, otherwise - comment this assert line\"\n",
    "\n",
    "model_name = \"default#0\"\n",
    "checkpoint = torch.load(os.path.join(\"./chkp\", f\"{model_name}.pt\"))\n",
    "\n",
    "# Создаём те же классы, что и внутри чекпоинта\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model, optimizer = create_model_and_optimizer(...)\n",
    "\n",
    "scheduler = ...\n",
    "\n",
    "# Загружаем состояния из чекпоинта\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "losses = checkpoint['losses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "j29u0qg4jm9uo4sb5009zq",
    "execution_id": "71a59fd7-d8b2-4941-a734-6131e5e54eb0"
   },
   "source": [
    "## 4. Оценка результатов (***2 балл***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xscbzlik3ghdvsntx4r5q7"
   },
   "source": [
    "### 4.1 Генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "t0elwvv6f9cvb3ft4030c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def generate(\n",
    "    model,\n",
    "    image,\n",
    "    max_seq_len: Optional[int] = max_seq_len,\n",
    "    top_p: Optional[float] = None,\n",
    "    top_k: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    По картинке image генерируете текст моделью model либо пока не сгенерируете '<EOS>' токен, либо пока не сгенерируете max_seq_len токенов\n",
    "        top_k -> после получения предсказания оставляете первые top_k слов и сэмплируете случайно с перенормированными вероятностями из оставшихся слов\n",
    "        top_p -> после получения предсказания оставляете первые сколько-то слов, так, чтобы суммарная вероятность оставшихся слов была не больше top_p,\n",
    "            после чего сэмплируете с перенормированными вероятностями из оставшихся слов\n",
    "        иначе -> сэмплируете случайное слово с предсказанными вероятностями\n",
    "    \"\"\"\n",
    "    assert top_p is None or top_k is None, \"Don't use top_p and top_k at the same time\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ...\n",
    "        return result_tokens, result_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "is9vm1z9qy7w0bwyc4p7",
    "execution_id": "54565491-9f72-46ca-9e05-4018dcf53535"
   },
   "source": [
    "### 4.2 Посмотрим на предсказания модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrap_text(text, max_width):\n",
    "    words = text.split(\" \")\n",
    "    result = [[words[0]]]\n",
    "    for word in words[1:]:\n",
    "        if len(\" \".join(result[-1])) + len(word) + 1 > max_width:\n",
    "            result[-1] = \" \".join(result[-1])\n",
    "            result.append([])\n",
    "        result[-1].append(word)\n",
    "    result[-1] = \" \".join(result[-1])\n",
    "    return \"\\n\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "8v6ghj16vdru7mzqhsodsd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "inds = list(range(10))\n",
    "split = 'train'\n",
    "\n",
    "h, w = 2, 5\n",
    "title_width = 43\n",
    "\n",
    "assert h*w >= len(inds)\n",
    "\n",
    "fig, ax = plt.subplots(h, w, figsize=(20, 15))\n",
    "\n",
    "for i, ind in enumerate(inds):\n",
    "    row = dfs[split].iloc[ind]\n",
    "    img_id = row['img_id']\n",
    "    img = cv2.imread(os.path.join(data_folder, split, img_id))\n",
    "    \n",
    "    _, pred_caption = generate(model, img)\n",
    "    pred_caption = wrap_text(pred_caption, title_width)\n",
    "    \n",
    "    captions = [row[f'caption #{i}'] for i in range(5)]\n",
    "    caption_adjasted = \"\\n\".join([f\"{i}: \" + wrap_text(caption, title_width) for i, caption in enumerate(captions)])\n",
    "    \n",
    "    caption = 'pred:\\n' + pred_caption + '\\n\\n' + caption_adjasted\n",
    "    \n",
    "    plt.subplot(h, w, i+1)\n",
    "    plt.title(caption)\n",
    "    plt.imshow(img)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "7l3qdo5j4uwl46mr44brvk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "inds = list(range(10))\n",
    "split = 'val'\n",
    "\n",
    "h, w = 2, 5\n",
    "title_width = 43\n",
    "\n",
    "top_p = None\n",
    "top_k = 3\n",
    "\n",
    "assert h*w >= len(inds)\n",
    "\n",
    "fig, ax = plt.subplots(h, w, figsize=(20, 15))\n",
    "\n",
    "for i, ind in enumerate(inds):\n",
    "    row = dfs[split].iloc[ind]\n",
    "    img_id = row['img_id']\n",
    "    img = cv2.imread(os.path.join(data_folder, split, img_id))\n",
    "    \n",
    "    _, pred_caption = generate(model, img)\n",
    "    pred_caption = wrap_text(pred_caption, title_width)\n",
    "    \n",
    "    captions = [row[f'caption #{i}'] for i in range(5)]\n",
    "    caption_adjasted = \"\\n\".join([f\"{i}: \" + wrap_text(caption, title_width) for i, caption in enumerate(captions)])\n",
    "    \n",
    "    caption = 'pred:\\n' + pred_caption + '\\n\\n' + caption_adjasted\n",
    "    \n",
    "    plt.subplot(h, w, i+1)\n",
    "    plt.title(caption)\n",
    "    plt.imshow(img)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 BLEU на всём val датасете"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут код уже написан, эта функция будет использоваться в контесте для проверки ваших результатов.\n",
    "\n",
    "Используйте её для само-проверки на валидационном сете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def get_bleu(model, top_p=None, top_k=None, split=\"val\"):\n",
    "    candidates = []\n",
    "    references = []\n",
    "    for ind, row in tqdm(dfs[split].iterrows(), total=dfs[split].shape[0]):\n",
    "        img_id = row['img_id']\n",
    "        img = cv2.imread(os.path.join(data_folder, split, img_id))\n",
    "        references.append([tokenize(row[f'caption #{j}'])[1:-1] for j in range(5)])\n",
    "        candidates.append(generate(model, img, top_p=top_p, top_k=top_k)[0])\n",
    "\n",
    "    # Здесь считается взвешенный BLEU score по 1-4-граммам, оставьте именно так\n",
    "    b1, b2, b3, b4 = (\n",
    "        bleu_score(candidates, references, weights=(1, 0, 0, 0)),\n",
    "        bleu_score(candidates, references, weights=(0, 1, 0, 0)),\n",
    "        bleu_score(candidates, references, weights=(0, 0, 1, 0)),\n",
    "        bleu_score(candidates, references, weights=(0, 0, 0, 1)),\n",
    "    )\n",
    "    return (\n",
    "        # (b1, b2, b3, b4),\n",
    "        (b1 + b2 + b3 + b4) / 4,  # Вам нужно добиться большого вот этого значения (см раздел 5.)\n",
    "        # bleu_score(candidates, references),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В том числе экспериментируйте с top_p, top_k и прочим\n",
    "bleu_res = get_bleu(model, top_p=None, top_k=None)\n",
    "print(bleu_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7hxkzdv19uobw6u30cjb",
    "execution_id": "d4e2d43d-99dd-43af-b9bc-7bf791670071"
   },
   "source": [
    "## 5. Эксперименты (***3+ баллов***)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "w6j0hab0z2m0b5nkp4eqs",
    "execution_id": "af8e5d64-1d86-4f69-90c1-fa39b93bd151"
   },
   "source": [
    "В этой части у вас не будет никакого написанного мною кода, а всё что вы здесь будете делать - на ваше усмотрение и по вашей задумке\n",
    "\n",
    "Цель эксперментов:\n",
    "**Пройти порог score в соревновании на отложенном тестовом сете**\n",
    "\n",
    "На Test датасете вам нужно преодолеть следующие пороги:\n",
    "\n",
    "**0.1** --> **1 балл**\n",
    "\n",
    "**0.2** --> **2 балла**\n",
    "\n",
    "**0.3** --> **3 балла**\n",
    "\n",
    "В зависимости от решений других студентов пороги могут измениться, а за самые лучшие решения мы можем добавить дополнительные баллы.\n",
    "\n",
    "**Удачи!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "db54o3dw3b5ddeygdutp75"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_py3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "notebookId": "9c48be40-b234-48dc-b300-7d064e09147d",
  "notebookPath": "Lab1 - CV + NLP/lab4_real_master.ipynb",
  "ydsNotebookPath": "Lab1 - CV + NLP/lab1_master.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
